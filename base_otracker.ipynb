{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d9d7d8",
   "metadata": {},
   "source": [
    "# Project part E - Refactored with ObjectTracker Class\n",
    "\n",
    "Static object tracking through multiple frames using SAM and DINOv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930be79c",
   "metadata": {},
   "source": [
    "## Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a9e913c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jovyan/venv_project/lib/python3.10/site-packages (25.2)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch==2.4.0+cu121 in /home/jovyan/venv_project/lib/python3.10/site-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: torchvision==0.19.0+cu121 in /home/jovyan/venv_project/lib/python3.10/site-packages (0.19.0+cu121)\n",
      "Requirement already satisfied: filelock in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torch==2.4.0+cu121) (3.0.0)\n",
      "Requirement already satisfied: numpy in /home/jovyan/venv_project/lib/python3.10/site-packages (from torchvision==0.19.0+cu121) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from torchvision==0.19.0+cu121) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jovyan/venv_project/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0+cu121) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu121) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu121) (1.3.0)\n",
      "Requirement already satisfied: transformers==4.44.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (4.44.0)\n",
      "Requirement already satisfied: huggingface-hub==0.24.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: pillow in /home/jovyan/venv_project/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy in /home/jovyan/venv_project/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: opencv-python in /home/jovyan/venv_project/lib/python3.10/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: open3d in /home/jovyan/venv_project/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: ipympl in /home/jovyan/venv_project/lib/python3.10/site-packages (0.9.7)\n",
      "Requirement already satisfied: rerun-sdk[notebook] in /home/jovyan/venv_project/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jovyan/venv_project/lib/python3.10/site-packages (from transformers==4.44.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from huggingface-hub==0.24.0) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jovyan/venv_project/lib/python3.10/site-packages (from huggingface-hub==0.24.0) (4.15.0)\n",
      "Requirement already satisfied: dash>=2.6.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (3.2.0)\n",
      "Requirement already satisfied: werkzeug>=3.0.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (3.1.3)\n",
      "Requirement already satisfied: flask>=3.0.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (3.1.2)\n",
      "Requirement already satisfied: nbformat>=5.7.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (5.10.4)\n",
      "Requirement already satisfied: configargparse in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (1.7.1)\n",
      "Requirement already satisfied: ipywidgets>=8.0.4 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (8.1.7)\n",
      "Requirement already satisfied: addict in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (2.4.0)\n",
      "Requirement already satisfied: matplotlib>=3 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (3.10.6)\n",
      "Requirement already satisfied: pandas>=1.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21 in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (1.7.2)\n",
      "Requirement already satisfied: pyquaternion in /home/jovyan/venv_project/lib/python3.10/site-packages (from open3d) (0.9.9)\n",
      "Requirement already satisfied: ipython<10 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipympl) (8.37.0)\n",
      "Requirement already satisfied: traitlets<6 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipympl) (5.14.3)\n",
      "Requirement already satisfied: decorator in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipython<10->ipympl) (0.6.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (0.2.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/jovyan/venv_project/lib/python3.10/site-packages (from ipywidgets>=8.0.4->open3d) (3.0.15)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jovyan/venv_project/lib/python3.10/site-packages (from matplotlib>=3->open3d) (2.9.0.post0)\n",
      "Requirement already satisfied: wcwidth in /home/jovyan/venv_project/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<10->ipympl) (0.2.14)\n",
      "Requirement already satisfied: attrs>=23.1.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from rerun-sdk[notebook]) (25.3.0)\n",
      "Requirement already satisfied: pyarrow>=18.0.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from rerun-sdk[notebook]) (21.0.0)\n",
      "Requirement already satisfied: rerun-notebook==0.25.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from rerun-sdk[notebook]) (0.25.1)\n",
      "Requirement already satisfied: anywidget in /home/jovyan/venv_project/lib/python3.10/site-packages (from rerun-notebook==0.25.1->rerun-sdk[notebook]) (0.9.18)\n",
      "Requirement already satisfied: jupyter-ui-poll in /home/jovyan/venv_project/lib/python3.10/site-packages (from rerun-notebook==0.25.1->rerun-sdk[notebook]) (1.0.0)\n",
      "Requirement already satisfied: plotly>=5.0.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (6.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jovyan/venv_project/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (8.7.0)\n",
      "Requirement already satisfied: retrying in /home/jovyan/venv_project/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.4.2)\n",
      "Requirement already satisfied: nest-asyncio in /home/jovyan/venv_project/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/venv_project/lib/python3.10/site-packages (from dash>=2.6.0->open3d) (65.5.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/jovyan/venv_project/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (8.3.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /home/jovyan/venv_project/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from flask>=3.0.0->open3d) (2.1.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jedi>=0.16->ipython<10->ipympl) (0.8.5)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/jovyan/venv_project/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/jovyan/venv_project/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (4.25.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/jovyan/venv_project/lib/python3.10/site-packages (from nbformat>=5.7.0->open3d) (5.8.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.27.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/jovyan/venv_project/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jovyan/venv_project/lib/python3.10/site-packages (from pandas>=1.0->open3d) (2025.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jovyan/venv_project/lib/python3.10/site-packages (from pexpect>4.3->ipython<10->ipympl) (0.7.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jovyan/venv_project/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from scikit-learn>=0.21->open3d) (3.6.0)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from anywidget->rerun-notebook==0.25.1->rerun-sdk[notebook]) (0.14.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/jovyan/venv_project/lib/python3.10/site-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jovyan/venv_project/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jovyan/venv_project/lib/python3.10/site-packages (from requests->transformers==4.44.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/venv_project/lib/python3.10/site-packages (from requests->transformers==4.44.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jovyan/venv_project/lib/python3.10/site-packages (from requests->transformers==4.44.0) (2025.8.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/jovyan/venv_project/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/jovyan/venv_project/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4170c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data, load_camera_intrinsics\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.model_loaders import load_sam_model, load_clip_model, load_dino_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae4e82",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81e0082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Project Configuration\n",
    "    PROJECT_CONFIG = {\n",
    "        'frame_skip': 7,                                  \n",
    "        'max_frames': 40,                                  \n",
    "        'grid_size': 6,                                    \n",
    "        'sam_confidence_threshold': 0.5,                   \n",
    "        'clip_model': 'openai/clip-vit-base-patch32',      \n",
    "        'example_viz_index': 29,                          \n",
    "        'padding_ratio_image_crops': 0.1                   \n",
    "    }\n",
    "    \n",
    "    # Tracking Configuration (NEW)\n",
    "    TRACKING_CONFIG = {\n",
    "        'similarity_threshold': 0.6,      # Minimum score to consider a match\n",
    "        'feature_weight': 0.8,            # Weight for feature similarity (alpha)\n",
    "        'spatial_weight': 0.2,            # Weight for spatial similarity (1-alpha)\n",
    "        'moving_target': False,           # Enable target updates\n",
    "        'update_momentum': 0.1,           # How much to update target (0.1 = conservative)\n",
    "        'confidence_threshold': 0.8       # Minimum confidence for target update\n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration (for visualization)\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'depth_scale': 1000.0,\n",
    "        'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04,\n",
    "        'batch_size': 20,\n",
    "        'max_frames': 1000,\n",
    "        'volume_length': 30.0,\n",
    "        'resolution': 512,\n",
    "    }\n",
    "\n",
    "    # GT and Display Configuration\n",
    "    GT_CONFIG = {\n",
    "        'allowed_classes': None, 'mesh_downsample_points': 75000,\n",
    "        'show_mesh': True, 'show_annotations': True\n",
    "    }\n",
    "    \n",
    "    # Rerun Viewer Dimensions\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 700\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43129b8f",
   "metadata": {},
   "source": [
    "## Core Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb543",
   "metadata": {},
   "source": [
    "### Object Tracker Class\n",
    "\n",
    "The `ObjectTracker` class encapsulates all the tracking logic in a clean interface. It handles:\n",
    "\n",
    "- **Target Management**: Setting and updating the reference object features\n",
    "- **Configuration**: Using the centralized tracking config for all parameters\n",
    "- **Similarity Matching**: Combining DINOv2 features with 3D spatial information  \n",
    "- **Moving Target Updates**: Optionally adapting to appearance changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "64596c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    \"\"\"Lightweight object tracker for multi-frame tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.target_features = None\n",
    "        self.target_bbox = None\n",
    "        self.target_centroid = None\n",
    "        self.tracking_history = []  # Optional: for debugging/analysis\n",
    "        \n",
    "    def set_target(self, features, bbox_3d, centroid):\n",
    "        \"\"\"Set the initial target to track.\"\"\"\n",
    "        self.target_features = features.copy() if hasattr(features, 'copy') else np.array(features)\n",
    "        self.target_bbox = bbox_3d.copy() if hasattr(bbox_3d, 'copy') else np.array(bbox_3d)\n",
    "        self.target_centroid = centroid.copy() if hasattr(centroid, 'copy') else np.array(centroid)\n",
    "        \n",
    "    def find_best_match(self, objects):\n",
    "        \"\"\"Find the best matching object in the current frame.\"\"\"\n",
    "        if self.target_features is None:\n",
    "            return None\n",
    "            \n",
    "        if not objects:\n",
    "            return None\n",
    "            \n",
    "        best_idx = None\n",
    "        best_score = 0.0\n",
    "        best_sim_features = 0.0  \n",
    "        best_sim_spatial = 0.0\n",
    "        similarity_threshold = self.config.TRACKING_CONFIG['similarity_threshold']\n",
    "        alpha = self.config.TRACKING_CONFIG['feature_weight']\n",
    "\n",
    "        for i, obj in enumerate(objects):\n",
    "\n",
    "            if isinstance(obj, ObjectTracker):\n",
    "                # Object is another tracker\n",
    "                features = obj.target_features\n",
    "                bbox_3d = obj.target_bbox\n",
    "                centroid = obj.target_centroid\n",
    "            else:\n",
    "                # Candidate is a dict\n",
    "                features = obj['features']\n",
    "                bbox_3d = obj['bbox_3d']\n",
    "                centroid = obj['centroid']\n",
    "            \n",
    "            # Hard distance cutoff \n",
    "            target_centroid = (self.target_bbox[0] + self.target_bbox[1]) / 2.0\n",
    "            distance = np.linalg.norm(centroid - target_centroid)\n",
    "            if distance > 0.5:  # Hard threshold\n",
    "                continue\n",
    "            \n",
    "            # Feature similarity (cosine similarity)\n",
    "            sim_features = np.dot(self.target_features, features) / (\n",
    "                np.linalg.norm(self.target_features) * np.linalg.norm(features) + 1e-8\n",
    "            )\n",
    "            \n",
    "            # Spatial similarity (3D IoU)\n",
    "            sim_spatial = self._bbox3d_iou(self.target_bbox, bbox_3d)\n",
    "\n",
    "            # Combined score\n",
    "            score = alpha * sim_features + (1 - alpha) * sim_spatial\n",
    "            \n",
    "            if score > best_score and score >= similarity_threshold:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "                best_sim_features = sim_features  \n",
    "                best_sim_spatial = sim_spatial   \n",
    "                \n",
    "        if best_idx is not None:\n",
    "            match_result = {\n",
    "                'index': best_idx, \n",
    "                'score': best_score,\n",
    "                'feature_sim': best_sim_features,\n",
    "                'spatial_sim': best_sim_spatial\n",
    "            }\n",
    "            #self.tracking_history.append(match_result)\n",
    "            return match_result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def update_target(self, new_features, new_bbox, confidence_score):\n",
    "        \"\"\"Update target with new features (moving target).\"\"\"\n",
    "\n",
    "        print(\"Appending to tracking_history\")\n",
    "        self.tracking_history.append([self.target_features, self.target_bbox])\n",
    "        #if not self.config.TRACKING_CONFIG['moving_target']:\n",
    "        #    return  # Moving target disabled\n",
    "            \n",
    "        confidence_threshold = self.config.TRACKING_CONFIG['confidence_threshold']\n",
    "        if confidence_score < confidence_threshold:\n",
    "            return  # Not confident enough to update        \n",
    "            \n",
    "        momentum = self.config.TRACKING_CONFIG['update_momentum']\n",
    "        \n",
    "        # Weighted update - keeps some \"memory\" of original target\n",
    "        self.target_features = (1 - momentum) * self.target_features + momentum * new_features\n",
    "        self.target_bbox = (1 - momentum) * self.target_bbox + momentum * new_bbox\n",
    "        \n",
    "\n",
    "    \n",
    "    def _bbox3d_iou(self, b1, b2):\n",
    "        \"\"\"Calculate 3D IoU between two bounding boxes.\"\"\"\n",
    "        b1_min, b1_max = b1[0], b1[1]\n",
    "        b2_min, b2_max = b2[0], b2[1]\n",
    "\n",
    "        x_min = max(b1_min[0], b2_min[0])\n",
    "        y_min = max(b1_min[1], b2_min[1])\n",
    "        z_min = max(b1_min[2], b2_min[2])\n",
    "        x_max = min(b1_max[0], b2_max[0])\n",
    "        y_max = min(b1_max[1], b2_max[1])\n",
    "        z_max = min(b1_max[2], b2_max[2])\n",
    "        \n",
    "        if x_min >= x_max or y_min >= y_max or z_min >= z_max:\n",
    "            return 0.0\n",
    "\n",
    "        inter_vol = (x_max - x_min) * (y_max - y_min) * (z_max - z_min)\n",
    "        vol1 = (b1_max - b1_min).prod()\n",
    "        vol2 = (b2_max - b2_min).prod()\n",
    "        \n",
    "        return inter_vol / (vol1 + vol2 - inter_vol + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35970090",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fe462f",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6da01",
   "metadata": {},
   "source": [
    "### SAM Proposals\n",
    "\n",
    "We filter proposals by confidence threshold and remove duplicates to get clean object candidates.\n",
    "\n",
    "Here we use SAM (Segment Anything Model) with a grid of point prompts, same approach as in Lab2 to generate object proposals.\n",
    "\n",
    "Each proposal contains:\n",
    "- **point**: Grid point that generated this proposal\n",
    "- **confidence**: SAM's confidence score for this segmentation\n",
    "- **mask**: Binary segmentation mask for the object\n",
    "- **area**: Number of pixels in the mask (for filtering small objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c66af372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sam_proposals_project(image: Image.Image,\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          grid_size: int = 6,\n",
    "                          confidence_threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Generate object proposals using SAM with a grid of point prompts.\"\"\"\n",
    "    width, height = image.size\n",
    "    # Generate grid of point prompts\n",
    "    x_points = np.linspace(width * 0.1, width * 0.9, grid_size)\n",
    "    y_points = np.linspace(height * 0.1, height * 0.9, grid_size)\n",
    "    \n",
    "    proposals = []\n",
    "    processed_masks = []\n",
    "    \n",
    "    tqdm.write(f\"Generating SAM proposals with {grid_size}x{grid_size} grid...\")\n",
    "    \n",
    "    for i, x in enumerate(x_points):\n",
    "        for j, y in enumerate(y_points):\n",
    "            input_points = [[[x, y]]]\n",
    "            \n",
    "            try:\n",
    "                inputs = sam_processor(\n",
    "                    images=image,\n",
    "                    input_points=input_points,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = sam_model(**inputs)\n",
    "                \n",
    "                masks = sam_processor.image_processor.post_process_masks(\n",
    "                    outputs.pred_masks.cpu(),\n",
    "                    inputs[\"original_sizes\"].cpu(),\n",
    "                    inputs[\"reshaped_input_sizes\"].cpu()\n",
    "                )\n",
    "                \n",
    "                batch_masks = masks[0]\n",
    "                if len(batch_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                point_masks = batch_masks[0]\n",
    "                if len(point_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                best_mask_idx = 0  # Default to first mask\n",
    "                best_score = 0.5   # Default confidence\n",
    "                \n",
    "                if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "                    try:\n",
    "                        iou_scores = outputs.iou_scores[0,0,:].cpu().numpy()\n",
    "                        if len(iou_scores) > 0:\n",
    "                            best_mask_idx = int(np.argmax(iou_scores))\n",
    "                            best_score = float(iou_scores[best_mask_idx])\n",
    "                             \n",
    "                            if best_score < confidence_threshold:\n",
    "                                continue\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                mask = point_masks[best_mask_idx]\n",
    "                if isinstance(mask, torch.Tensor):\n",
    "                    mask_np = mask.cpu().numpy().astype(bool)\n",
    "                else:\n",
    "                    mask_np = np.array(mask).astype(bool)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                is_duplicate = False\n",
    "                for existing_mask in processed_masks:\n",
    "                    overlap = np.sum(mask_np & existing_mask)\n",
    "                    union = np.sum(mask_np | existing_mask)\n",
    "                    if union > 0 and overlap / union > 0.8:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate and np.sum(mask_np) > 100:\n",
    "                    proposals.append({\n",
    "                        'mask': mask_np,\n",
    "                        'area': np.sum(mask_np),\n",
    "                        'point': [x, y],\n",
    "                        'confidence': best_score\n",
    "                    })\n",
    "                    processed_masks.append(mask_np)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    tqdm.write(f\"Generated {len(proposals)} unique segment proposals\")\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a1f74",
   "metadata": {},
   "source": [
    "### CLIP Features\n",
    "\n",
    "These features are used for initial target selection when a text query is provided.\n",
    "\n",
    "We extract CLIP features from segmented regions to enable text-based object queries (like \"ball\" or \"sofa\").\n",
    "\n",
    "The process:\n",
    "1. **Make bounding box** of the mask\n",
    "2. **Crop the image** to the bounding box\n",
    "3. **Expand with padding** to include context around the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1e5b2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clip_features_from_segment(image: Image.Image,\n",
    "                                  mask: np.ndarray,\n",
    "                                  clip_model,\n",
    "                                  clip_processor,\n",
    "                                  device: str,\n",
    "                                  padding_ratio: float = 0.1) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract CLIP features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((int(x_min_exp), int(y_min_exp), int(x_max_exp), int(y_max_exp)))\n",
    "                \n",
    "        # Process with CLIP\n",
    "        inputs = clip_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "            \n",
    "            # Normalize image features\n",
    "            image_features = image_features/ image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract CLIP features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b27db39",
   "metadata": {},
   "source": [
    "### DINOv2 Features\n",
    "\n",
    "\n",
    "DINOv2 provides superior visual features for object tracking compared to CLIP, especially for appearance-based matching across frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7ca3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dino_features_from_segment(\n",
    "    image: Image.Image,\n",
    "    mask: np.ndarray,\n",
    "    dino_model,\n",
    "    dino_processor,\n",
    "    device: str,\n",
    "    padding_ratio: float = 0.1\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract DINOv2 features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((x_min_exp, y_min_exp, x_max_exp, y_max_exp))\n",
    "        \n",
    "        # Process with DINOv2\n",
    "        inputs = dino_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "            # Use the [CLS] token embedding or pooled output as image feature\n",
    "            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                image_features = outputs.pooler_output\n",
    "            else:\n",
    "                image_features = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract DINO features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f814f5",
   "metadata": {},
   "source": [
    "## 3D Position Extraction\n",
    "\n",
    "To enable spatial tracking in world coordinates, we convert 2D segmentation masks into 3D object representations using camera geometry and depth data.\n",
    "\n",
    "#### **What We Extract:**\n",
    "- **Object position**: 3D centroid in world coordinates  \n",
    "- **Object extent**: Axis-aligned bounding box for size/shape information\n",
    "- **Spatial context**: Used for 3D IoU calculations in tracking similarity\n",
    "\n",
    "#### **The Pipeline:**\n",
    "\n",
    "**1. Pixel Sampling**\n",
    "- **Sample up to 500 pixels** from the mask for computational efficiency\n",
    "- **Filter minimum size**: Skip masks with fewer than 20 pixels (noise rejection)\n",
    "\n",
    "**2. Depth Processing** \n",
    "- **Extract depth values** at sampled pixel locations\n",
    "- **Filter valid depths**: Remove pixels with depth ≤ 0 (invalid/missing data)\n",
    "\n",
    "**3. 3D Reconstruction**\n",
    "- **Back-project to camera space**: Use intrinsics (fx, fy, cx, cy) to convert pixels → 3D points\n",
    "- **Transform to world space**: Apply camera pose matrix for global coordinates\n",
    "\n",
    "**4. Compact Representation**\n",
    "- **Centroid**: Mean position of all valid 3D points\n",
    "- **Bounding box**: [min_xyz, max_xyz] extent for spatial similarity\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Efficient**: Sampling limits computation while preserving shape information\n",
    "- **Robust**: Filters invalid depth data that could corrupt 3D positions  \n",
    "- **Tracking-ready**: Compact format perfect for 3D IoU calculations between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb948f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_3d_position_from_mask(\n",
    "    mask: np.ndarray,\n",
    "    depth_image: np.ndarray,\n",
    "    camera_intrinsics: np.ndarray,\n",
    "    camera_pose: np.ndarray,\n",
    "    depth_scale: float,\n",
    "    max_points: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert a binary mask into compact 3D object representation:\n",
    "    centroid + bounding box in world coordinates.\n",
    "\n",
    "    Args:\n",
    "        mask: Binary segmentation mask (H,W)\n",
    "        depth_image: Depth map aligned with RGB (H,W), uint16 or float\n",
    "        camera_intrinsics\n",
    "        camera_pose\n",
    "        depth_scale: scale factor for depth values\n",
    "        max_points: number of pixels to sample from mask\n",
    "\n",
    "    Returns:\n",
    "        dict with 'centroid' and 'bbox_3d', or None if invalid\n",
    "    \"\"\"\n",
    "    coords = np.column_stack(np.where(mask))  # (y, x) pixel indices\n",
    "    if coords.shape[0] < 20:\n",
    "        return None\n",
    "\n",
    "    # Sample pixels if mask too large\n",
    "    sample_idx = np.random.choice(len(coords), min(max_points, len(coords)), replace=False)\n",
    "    coords = coords[sample_idx]\n",
    "\n",
    "    # Depth values\n",
    "    z = depth_image[coords[:, 0], coords[:, 1]].astype(np.float32) / depth_scale\n",
    "    valid = z > 0\n",
    "    if not np.any(valid):\n",
    "        return None\n",
    "\n",
    "    coords = coords[valid]\n",
    "    z = z[valid]\n",
    "\n",
    "    # Get camera parameters\n",
    "    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
    "    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
    "    \n",
    "    x = (coords[:, 1] - cx) * z / fx\n",
    "    y = (coords[:, 0] - cy) * z / fy\n",
    "    pts_cam = np.stack([x, y, z], axis=-1)\n",
    "\n",
    "    # Camera → World\n",
    "    pts_h = np.concatenate([pts_cam, np.ones((pts_cam.shape[0], 1))], axis=-1).T\n",
    "    pts_world = (camera_pose @ pts_h)[:3].T\n",
    "\n",
    "    # Compact representation\n",
    "    centroid = pts_world.mean(axis=0)\n",
    "    bbox_min = pts_world.min(axis=0)\n",
    "    bbox_max = pts_world.max(axis=0)\n",
    "\n",
    "    return {\n",
    "        'centroid': centroid,\n",
    "        'bbox_3d': np.stack([bbox_min, bbox_max])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c944aa7",
   "metadata": {},
   "source": [
    "## Frame Processing\n",
    "\n",
    "Each processed frame contains a list of detected objects with their visual features, spatial location, and metadata.\n",
    "\n",
    "This is where we put it all together for each frame:\n",
    "\n",
    "1. **Load RGB and depth images** for the frame\n",
    "\n",
    "2. **Generate SAM proposals** using the grid-based approach\n",
    "3. **Extract features** for each proposal (both DINOv2 and CLIP)\n",
    "4. **Calculate 3D positions** from masks and depth data\n",
    "5. **Package results** into a structured format for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ed1a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_project(frame_data: Dict,\n",
    "        sam_model, sam_processor,\n",
    "        dino_model, dino_processor,\n",
    "        clip_model, clip_processor,\n",
    "        device: str,\n",
    "        config: Config,\n",
    "        grid_size: int = 6) -> Dict:\n",
    "    \"\"\"Process a single frame for Project: SAM proposals + CLIP embeddings + DINOv2 features.\"\"\"\n",
    "    results = {\n",
    "        'frame_name': frame_data['frame_name'],\n",
    "        'objects': []\n",
    "    }\n",
    "    try:\n",
    "        # Load images\n",
    "        image = Image.open(frame_data['rgb_path']).convert(\"RGB\")\n",
    "        depth_image = cv2.imread(frame_data['depth_path'], cv2.IMREAD_UNCHANGED)\n",
    "        rgb_image = np.array(image)\n",
    "        \n",
    "        # Generate SAM proposals\n",
    "        proposals = generate_sam_proposals_project(\n",
    "            image,\n",
    "            sam_model,\n",
    "            sam_processor,\n",
    "            device,\n",
    "            grid_size=grid_size,\n",
    "            confidence_threshold=config.PROJECT_CONFIG['sam_confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        # Process each proposal\n",
    "        for proposal in proposals:\n",
    "            mask = proposal['mask']\n",
    "            \n",
    "            # Extract DINOv2 features\n",
    "            dino_features = extract_dino_features_from_segment(\n",
    "                image,\n",
    "                mask,\n",
    "                dino_model,\n",
    "                dino_processor,\n",
    "                device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops'] \n",
    "            )\n",
    "            \n",
    "            if dino_features is None:\n",
    "                continue\n",
    "\n",
    "            clip_features = extract_clip_features_from_segment(\n",
    "                image, mask, clip_model, clip_processor, device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops']\n",
    "            )\n",
    "            \n",
    "            # Get compact 3D position (Bounding 3D box + Centroid)\n",
    "            pos_data = extract_3d_position_from_mask(\n",
    "                mask,\n",
    "                depth_image,\n",
    "                frame_data['camera_intrinsics'],\n",
    "                frame_data['camera_pose'],\n",
    "                config.TSDF_CONFIG['depth_scale']\n",
    "            )\n",
    "            if pos_data is None:\n",
    "                continue\n",
    "            \n",
    "            results['objects'].append({\n",
    "                'features': dino_features,\n",
    "                'clip_features': clip_features,\n",
    "                'centroid': pos_data['centroid'],\n",
    "                'bbox_3d': pos_data['bbox_3d'],\n",
    "                'area_px': proposal['area'],\n",
    "                'confidence': proposal['confidence']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_data['frame_name']}: {e}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a826da",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe19365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 540 camera poses\n"
     ]
    }
   ],
   "source": [
    "# Load camera poses\n",
    "camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "\n",
    "def get_frame_list(config, frame_skip=1, max_frames=25, start_frame_name=None):\n",
    "    files = sorted(os.listdir(config.RGB_PATH))\n",
    "    frames = []\n",
    "\n",
    "    # If start_frame_name is provided, find its index\n",
    "    if start_frame_name is not None:\n",
    "        try:\n",
    "            start_idx = next(i for i, f in enumerate(files) if f == start_frame_name)\n",
    "            files = files[start_idx:]  # start from that frame\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"Start frame {start_frame_name} not found in {config.RGB_PATH}\")\n",
    "\n",
    "    for i, f in enumerate(files[::frame_skip]):\n",
    "        if i >= max_frames:\n",
    "            break\n",
    "\n",
    "        timestamp = os.path.splitext(f)[0]\n",
    "\n",
    "        # Load intrinsics for this frame\n",
    "        try:\n",
    "            K, image_size = load_camera_intrinsics(config.INTRINSICS_PATH, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}, failed to load intrinsics: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load camera pose\n",
    "        pose = camera_poses.get(timestamp, np.eye(4))\n",
    "\n",
    "        frames.append({\n",
    "            \"frame_name\": f,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"rgb_path\": os.path.join(config.RGB_PATH, f),\n",
    "            \"depth_path\": os.path.join(config.DEPTH_PATH, f.replace(\".jpg\", \".png\")),\n",
    "            \"camera_intrinsics\": K,\n",
    "            \"intrinsics_size\": image_size,\n",
    "            \"camera_pose\": pose\n",
    "        })\n",
    "\n",
    "    return frames\n",
    "\n",
    "def project_points(K, points_3d):\n",
    "    \"\"\"Project 3D points to 2D for visualization.\"\"\"\n",
    "    uv_coords = []\n",
    "    for X, Y, Z in points_3d:\n",
    "        if Z > 0:\n",
    "            uv = K @ np.array([X, Y, Z])\n",
    "            u, v = uv[0] / uv[2], uv[1] / uv[2]\n",
    "            uv_coords.append((u, v))\n",
    "    return uv_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8297e2",
   "metadata": {},
   "source": [
    "## High-Level Interface Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa443b0",
   "metadata": {},
   "source": [
    "### Frame Analysis\n",
    "\n",
    "The `analyse_frames()` function processes a sequence of frames to extract objects and their features for tracking.\n",
    "\n",
    "#### **Configuration:**\n",
    "- **n_frames**: Number of frames to process (default: 30)\n",
    "- **start_frame**: Starting frame filename (e.g., \"47333473_58534.757.png\")\n",
    "- **frame_skip**: Uses config setting to skip frames for efficiency\n",
    "\n",
    "#### **Processing Pipeline:**\n",
    "1. **Load models once** - SAM, DINOv2, and CLIP models loaded efficiently at startup\n",
    "2. **Get frame metadata** - Camera poses, intrinsics, and file paths for each frame\n",
    "3. **Process each frame** - Run the complete pipeline:\n",
    "   - Generate SAM object proposals\n",
    "   - Extract DINOv2 features for tracking\n",
    "   - Extract CLIP features for text queries\n",
    "   - Calculate 3D positions from depth data\n",
    "4. **Package results** - Structured data ready for the ObjectTracker\n",
    "\n",
    "#### **Output:**\n",
    "- **frame_results**: List of frames, each containing detected objects with features and 3D positions\n",
    "- **device**: GPU device handle for subsequent tracking operations\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Model reuse**: Load expensive models once, not per frame\n",
    "- **Structured output**: Consistent format for all tracking functions\n",
    "- **Flexible sequences**: Start from any frame, process any number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1390e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_frames(config, n_frames=30, start_frame=\"47333473_58549.751.png\"):\n",
    "    \"\"\"\n",
    "    Extract objects from multiple frames using SAM + DINO + CLIP.\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    sam_model, sam_processor, device = load_sam_model(model_size='base')\n",
    "    dino_model, dino_processor, _ = load_dino_model(device=device)\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "\n",
    "    # Get frame metadata\n",
    "    frames_metadata = get_frame_list(config, frame_skip=1, max_frames=n_frames, start_frame_name=start_frame)\n",
    "\n",
    "    all_results = []\n",
    "    for frame_data in tqdm(frames_metadata, desc=\"Processing frames\"):\n",
    "        results = process_frame_project(\n",
    "            frame_data,\n",
    "            sam_model, sam_processor,\n",
    "            dino_model, dino_processor,\n",
    "            clip_model, clip_processor,\n",
    "            device,\n",
    "            config\n",
    "        )\n",
    "        results[\"frame_data\"] = frame_data\n",
    "        all_results.append(results)\n",
    "\n",
    "    return all_results, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b641",
   "metadata": {},
   "source": [
    "### Tracking with Slider (Refactored)\n",
    "\n",
    "**Interactive Visualization**: The `query_with_slider()` function implements the complete tracking pipeline with an interactive slider for visual assessment. Your eyes are the ground truth here!\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "**1. Target Selection**\n",
    "- **With text query**: Uses CLIP similarity to find best matching object (e.g., \"ball\", \"sofa\")\n",
    "- **Without query**: Selects largest object (most pixels) for better tracking stability\n",
    "\n",
    "**2. Tracking Process**\n",
    "- **Initialize tracker** with target features and 3D bounding box from selected object\n",
    "- **For each frame**: Find best match using combined DINOv2 feature + 3D spatial similarity\n",
    "- **Update target** (if moving_target enabled) using exponential moving average for adaptation\n",
    "\n",
    "**3. Visualization**\n",
    "- **Bounding boxes** are projected from 3D world coordinates to 2D image space\n",
    "- **Interactive slider** lets you browse through frames to visually assess tracking quality\n",
    "- **Red rectangles** show the tracked object's projected 3D bounding box\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Visual assessment** is more reliable than automated metrics without ground truth\n",
    "- **Interactive exploration** lets you see exactly when tracking succeeds or fails\n",
    "- **3D-to-2D projection** provides spatial context from the world coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "70ab0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_slider(frame_results, device=None, query=None):\n",
    "    \"\"\"Track objects through frames with interactive slider - now using ObjectTracker class.\"\"\"\n",
    "    \n",
    "    if not frame_results:\n",
    "        print(\"Error: No frame results provided\")\n",
    "        return\n",
    "    \n",
    "    if not any(len(r['objects']) > 0 for r in frame_results):\n",
    "        print(\"Error: No objects found in any frame\")\n",
    "        return\n",
    "\n",
    "    if device is None:\n",
    "        _, _, device = load_sam_model(model_size='base')\n",
    "\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = ObjectTracker(config)\n",
    "\n",
    "    # Target selection logic\n",
    "    text_features_np = None\n",
    "    if query is not None:\n",
    "        text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features_np = text_features.cpu().numpy().squeeze()\n",
    "\n",
    "    # Pick object to track\n",
    "    if query is None or text_features_np is None:            \n",
    "        # Pick an object (first object with largest area for better tracking)\n",
    "        best_object_id, best_frame_id = 0, 0\n",
    "        best_area = 0\n",
    "        \n",
    "        # Look for substantial object in first few frames\n",
    "        for f_idx in range(min(3, len(frame_results))):\n",
    "            for o_idx, obj in enumerate(frame_results[f_idx]['objects']):\n",
    "                if obj['area_px'] > best_area:\n",
    "                    best_area = obj['area_px']\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "        \n",
    "    else:\n",
    "        # Pick object by CLIP similarity with query\n",
    "        best_score_overall = -1\n",
    "        best_frame_id, best_object_id = None, None\n",
    "        for f_idx, results in enumerate(frame_results):\n",
    "            for o_idx, obj in enumerate(results['objects']):\n",
    "                if obj['clip_features'] is None:\n",
    "                    continue\n",
    "                sim = float(np.dot(obj['clip_features'], text_features_np))\n",
    "                if sim > best_score_overall:\n",
    "                    best_score_overall = sim\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "\n",
    "        if best_score_overall < 0:\n",
    "            print(\"No suitable object found for query.\")\n",
    "            return\n",
    "\n",
    "    # Set initial target\n",
    "    target_features = frame_results[best_frame_id]['objects'][best_object_id]['features']\n",
    "    target_bbox = frame_results[best_frame_id]['objects'][best_object_id]['bbox_3d']\n",
    "    target_centroid = frame_results[best_frame_id]['objects'][best_object_id]['centroid']\n",
    "    tracker.set_target(target_features, target_bbox, target_centroid)\n",
    "    \n",
    "    print(f\"Starting tracking from frame {best_frame_id}, object {best_object_id}\")\n",
    "    print(f\"Tracking config: moving_target={config.TRACKING_CONFIG['moving_target']}, threshold={config.TRACKING_CONFIG['similarity_threshold']}\")\n",
    "\n",
    "    # Process frames with tracker\n",
    "    processed_frames = []\n",
    "    print(f\"Processing frames ...\")\n",
    "    \n",
    "    for idx, results in enumerate(frame_results):\n",
    "        frame = Image.open(results['frame_data']['rgb_path']).convert(\"RGB\")\n",
    "        overlay = frame.convert('RGBA')\n",
    "        draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "\n",
    "        if results['objects']:\n",
    "            # Use tracker to find best match\n",
    "            match = tracker.find_best_match(results['objects'])\n",
    "            if match:\n",
    "                best_idx = match['index']\n",
    "                obj = results['objects'][best_idx]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                points_2d = project_points(results['frame_data']['camera_intrinsics'], obj['bbox_3d'])\n",
    "                if points_2d:\n",
    "                    xs, ys = zip(*points_2d)\n",
    "                    x0, x1 = int(min(xs)), int(max(xs))\n",
    "                    y0, y1 = int(min(ys)), int(max(ys))\n",
    "                    draw.rectangle([x0, y0, x1, y1], outline=(255,0,0,180), width=3)\n",
    "                \n",
    "                # Update target if moving target is enabled\n",
    "                tracker.update_target(obj['features'], obj['bbox_3d'], match['score'])\n",
    "\n",
    "        processed_frames.append(overlay)\n",
    "\n",
    "\n",
    "\n",
    "    # Create slider widget\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(processed_frames)-1,\n",
    "        step=1,\n",
    "        description='Frame'\n",
    "    )\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def show_frame(change):\n",
    "        idx = change['new']\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            display(processed_frames[idx])\n",
    "\n",
    "    slider.observe(show_frame, names='value')\n",
    "    display(widgets.VBox([slider, out]))\n",
    "    \n",
    "    # Show first frame by default\n",
    "    with out:\n",
    "        display(processed_frames[0])\n",
    "    \n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4e5fd",
   "metadata": {},
   "source": [
    "## Examples and Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0019f",
   "metadata": {},
   "source": [
    "### Example 1: Track with Fixed Target (Default)\n",
    "\n",
    "**Trade-off**: May lose track if the object's appearance changes significantly, but won't accidentally switch to tracking a different object.\n",
    "\n",
    "This demonstrates the basic tracking approach where the target features **never change** after initialization.\n",
    "\n",
    "**Use case**: When you want the most stable tracking that won't drift to other objects, even if the appearance changes due to lighting or viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e749c308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading DINOv2 model: facebook/dinov2-base...\n",
      "DINOv2 model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venv_project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded on device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94bd45dfb7f4606b8a9e8114d9ef9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing frames:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 11 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 13 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 16 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 17 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 15 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 15 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 13 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 9 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 13 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 12 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 13 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 15 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 13 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n",
      "Generating SAM proposals with 6x6 grid...\n",
      "Generated 14 unique segment proposals\n"
     ]
    }
   ],
   "source": [
    "# Extract objects from frames\n",
    "frame_results, device_id = analyse_frames(config, n_frames=20, start_frame=\"47333473_58549.751.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "55aab30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tracking with Fixed Target ===\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venv_project/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 0, object 10\n",
      "Tracking config: moving_target=False, threshold=0.8\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c912aa179d04335a25a6053abca33c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=1), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track with query (fixed target)\n",
    "print(\"=== Tracking with Fixed Target ===\")\n",
    "tracker_fixed = query_with_slider(frame_results, device=None, query=\"pillow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22c7cf",
   "metadata": {},
   "source": [
    "### Example 2: Track with Moving Average Target\n",
    "\n",
    "**Trade-off**: Better handles appearance changes, but small risk of gradually drifting to a different object.\n",
    "\n",
    "Here we enable adaptive tracking where the target features **gradually update** based on successful matches.\n",
    "\n",
    "**Use case**: When lighting, shadows, or viewing angles change significantly across frames. The tracker adapts while maintaining object identity.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "- `moving_target = True`: Enable feature updates\n",
    "- `confidence_threshold = 0.8`: Only update when very confident\n",
    "- `update_momentum = 0.1`: Conservative update (10% new, 90% existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57133979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tracking with Moving Target ===\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 6, object 5\n",
      "Tracking config: moving_target=True, threshold=0.6\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0a48157eb349a794139356434d55c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable moving target\n",
    "config.TRACKING_CONFIG['moving_target'] = True\n",
    "config.TRACKING_CONFIG['update_momentum'] = 0.1  # Conservative update\n",
    "config.TRACKING_CONFIG['confidence_threshold'] = 0.8\n",
    "\n",
    "print(\"=== Tracking with Moving Target ===\")\n",
    "tracker_moving = query_with_slider(frame_results, device=None, query=\"ball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba6d86",
   "metadata": {},
   "source": [
    "### Example 3: Compare Tracking Settings\n",
    "\n",
    "**Compare by**: Using the sliders to see when each approach loses/maintains tracking. Look for tracking gaps, bounding box jumps, or switches to wrong objects.\n",
    "\n",
    "This example shows how different similarity thresholds affect tracking behavior. Each threshold gets its own interactive slider for direct comparison.\n",
    "**Similarity Thresholds:**-\n",
    "- **0.8 (Strict)**: Only high-confidence matches → fewer false positives but may lose track more easily\n",
    "- **0.6 (Balanced)**: Moderate threshold → good balance of continuity and accuracy  \n",
    "- **0.4 (Permissive)**: Accepts lower-confidence matches → more continuous tracking but higher false positive risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8535615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparing Different Similarity Thresholds ===\n",
      "\n",
      "--- Threshold 0.4 (Permissive) ---\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 6, object 6\n",
      "Tracking config: moving_target=False, threshold=0.4\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d058d7b84df1471f90ded45ce8297ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Threshold 0.6 (Balanced) ---\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 6, object 6\n",
      "Tracking config: moving_target=False, threshold=0.6\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855e026a84ad436a8c356362bdae60ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Threshold 0.8 (Strict) ---\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 6, object 6\n",
      "Tracking config: moving_target=False, threshold=0.8\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c21752096a4c5998dd48912353bf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset to fixed target for comparison\n",
    "config.TRACKING_CONFIG['moving_target'] = False\n",
    "\n",
    "# Try different similarity thresholds with separate sliders\n",
    "print(\"=== Comparing Different Similarity Thresholds ===\")\n",
    "\n",
    "original_threshold = config.TRACKING_CONFIG['similarity_threshold']\n",
    "\n",
    "# Threshold 0.4 - Permissive\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.4\n",
    "print(\"\\n--- Threshold 0.4 (Permissive) ---\")\n",
    "tracker_04 = query_with_slider(frame_results, device=None, query=\"bookshelf\")\n",
    "\n",
    "# Threshold 0.6 - Balanced  \n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.6\n",
    "print(\"\\n--- Threshold 0.6 (Balanced) ---\")\n",
    "tracker_06 = query_with_slider(frame_results, device=None, query=\"bookshelf\")\n",
    "\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = original_threshold\n",
    "\n",
    "# Threshold 0.8 - Strict# Reset to original\n",
    "\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.8\n",
    "\n",
    "print(\"\\n--- Threshold 0.8 (Strict) ---\")\n",
    "tracker_08 = query_with_slider(frame_results, device=None, query=\"bookshelf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74ad2",
   "metadata": {},
   "source": [
    "### Example 4: Track without Query (Largest Object)\n",
    "\n",
    "**Use case**: Exploratory analysis when you don't know what objects are present or want to track the most prominent object in the scene.\n",
    "\n",
    "When no text query is provided, the tracker automatically selects the largest detected object (by pixel area) as the target.\n",
    "\n",
    "\n",
    "**Why largest object?**- \n",
    "- **Less noise**: Larger masks are less likely to be false detections\n",
    "- **More stable tracking**: Larger objects have more feature information\n",
    "- **Better for furniture**: In indoor scenes, large objects like sofas/tables are good tracking targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "207f22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tracking Largest Object (No Query) ===\n",
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 0, object 8\n",
      "Tracking config: moving_target=False, threshold=0.8\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6dd0d02cf74dcd9ea8f58d41da4b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track without specific query - will pick largest object\n",
    "print(\"=== Tracking Largest Object (No Query) ===\")\n",
    "tracker_no_query = query_with_slider(frame_results, device=None, query=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a873f",
   "metadata": {},
   "source": [
    "## Configuration Toggle Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e0913811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tracking Configuration:\n",
      "  similarity_threshold: 0.8\n",
      "  feature_weight: 0.8\n",
      "  spatial_weight: 0.2\n",
      "  moving_target: False\n",
      "  update_momentum: 0.1\n",
      "  confidence_threshold: 0.8\n",
      "\n",
      "To modify tracking behavior:\n",
      "  config.TRACKING_CONFIG['moving_target'] = True/False\n",
      "  config.TRACKING_CONFIG['similarity_threshold'] = 0.4-0.9\n",
      "  config.TRACKING_CONFIG['feature_weight'] = 0.5-0.9 (vs spatial weight)\n",
      "  config.TRACKING_CONFIG['update_momentum'] = 0.05-0.3 (if moving_target=True)\n"
     ]
    }
   ],
   "source": [
    "# Show current configuration\n",
    "print(\"Current Tracking Configuration:\")\n",
    "for key, value in config.TRACKING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTo modify tracking behavior:\")\n",
    "print(\"  config.TRACKING_CONFIG['moving_target'] = True/False\")\n",
    "print(\"  config.TRACKING_CONFIG['similarity_threshold'] = 0.4-0.9\")\n",
    "print(\"  config.TRACKING_CONFIG['feature_weight'] = 0.5-0.9 (vs spatial weight)\")\n",
    "print(\"  config.TRACKING_CONFIG['update_momentum'] = 0.05-0.3 (if moving_target=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c5874243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox3d_to_world(bbox_3d, camera_pose):\n",
    "    \"\"\"\n",
    "    bbox_3d: np.array shape (2,3) - min/max corners in camera frame\n",
    "    camera_pose: 4x4 np.array - camera to world transform\n",
    "    Returns: np.array shape (2,3) - min/max corners in world frame\n",
    "    \"\"\"\n",
    "    # Compute 8 corners\n",
    "    corners = np.array([\n",
    "        [bbox_3d[0,0], bbox_3d[0,1], bbox_3d[0,2]],\n",
    "        [bbox_3d[1,0], bbox_3d[0,1], bbox_3d[0,2]],\n",
    "        [bbox_3d[0,0], bbox_3d[1,1], bbox_3d[0,2]],\n",
    "        [bbox_3d[0,0], bbox_3d[0,1], bbox_3d[1,2]],\n",
    "        [bbox_3d[1,0], bbox_3d[1,1], bbox_3d[0,2]],\n",
    "        [bbox_3d[1,0], bbox_3d[0,1], bbox_3d[1,2]],\n",
    "        [bbox_3d[0,0], bbox_3d[1,1], bbox_3d[1,2]],\n",
    "        [bbox_3d[1,0], bbox_3d[1,1], bbox_3d[1,2]],\n",
    "    ])\n",
    "    # Homogeneous coordinates\n",
    "    corners_h = np.hstack([corners, np.ones((8,1))])\n",
    "    corners_world_h = (camera_pose @ corners_h.T).T\n",
    "    corners_world = corners_world_h[:, :3]\n",
    "    # Return min/max\n",
    "    return np.array([corners_world.min(axis=0), corners_world.max(axis=0)])\n",
    "\n",
    "\n",
    "def prepare_detections_for_viz(frame_results, query_label=None):\n",
    "    detections_3d = []\n",
    "\n",
    "    for fr in frame_results:\n",
    "        camera_pose = fr['frame_data']['camera_pose']  # <- fixed here\n",
    "        for obj in fr['objects']:\n",
    "            # Optional label filtering\n",
    "            if query_label is not None and obj.get('label') != query_label:\n",
    "                continue\n",
    "\n",
    "            # Transform bbox to world coordinates\n",
    "            bbox_world = bbox3d_to_world(obj['bbox_3d'], camera_pose)\n",
    "\n",
    "            # Append to detection list\n",
    "            detections_3d.append({\n",
    "                'label': obj.get('label', 'unknown'),\n",
    "                'score': obj.get('confidence', 1.0),\n",
    "                'bbox_3d_world': bbox_world\n",
    "            })\n",
    "    return detections_3d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d4b9c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing: Objects in World Frame\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4c5de6bf2e457483136a5cc2734373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<div id=\"c77e410e-33a9-44a2-9a99-9f56e7d0b862\"><style onload=\"eval(atob(\\'KGFzeW5jIGZ1bmN0aW9uICgp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289a4b23eb244f2f8d4f957285f93903",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Viewer()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D visualization complete\n"
     ]
    }
   ],
   "source": [
    "detections_3d = prepare_detections_for_viz(frame_results)\n",
    "visualize_3d_scene_bbox_results(\n",
    "    point_cloud=None,  # or your frame point cloud\n",
    "    detections_3d=detections_3d,\n",
    "    gt_annotations=gt_annotations,\n",
    "    title=\"Objects in World Frame\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "53b0c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "51f873d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM model (base)...\n",
      "SAM model loaded on device: cuda\n",
      "Loading CLIP model: openai/clip-vit-base-patch32...\n",
      "CLIP model loaded on device: cuda\n",
      "Starting tracking from frame 2, object 8\n",
      "Tracking config: moving_target=False, threshold=0.8\n",
      "Processing frames ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b6977e837b43b9abd1d4a503838988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, description='Frame', max=19), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/EmbodiedAIProject/base_otracker.ipynb Cell 44\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tracker \u001b[39m=\u001b[39m query_with_slider(frame_results, query\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchair\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Convert tracking history to detection format\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m detections_3d \u001b[39m=\u001b[39m tracked_to_detections(frame_results, tracker)\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m visualize_3d_scene_bbox_results(\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     point_cloud\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,       \u001b[39m# optional: frame point cloud\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     detections_3d\u001b[39m=\u001b[39mdetections_3d,\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     gt_annotations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,    \u001b[39m# optional\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTracked Object in 3D\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/home/jovyan/EmbodiedAIProject/base_otracker.ipynb Cell 44\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     bbox \u001b[39m=\u001b[39m obj[\u001b[39m'\u001b[39m\u001b[39mbbox_3d\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# Transform each corner of bbox\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     bbox_h \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack([bbox, np\u001b[39m.\u001b[39;49mones((\u001b[39m8\u001b[39;49m,\u001b[39m1\u001b[39;49m))])  \u001b[39m# 8 corners\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     bbox_world \u001b[39m=\u001b[39m (pose \u001b[39m@\u001b[39m bbox_h\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39mT[:, :\u001b[39m3\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EmbodiedAIProject/base_otracker.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venv_project/lib/python3.10/site-packages/numpy/_core/shape_base.py:364\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39mdtype, casting\u001b[39m=\u001b[39mcasting)\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2 and the array at index 1 has size 8"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2949966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_objects(frame_results):\n",
    "    \"\"\"Track multiple objects across frames.\"\"\"\n",
    "    trackers = []\n",
    "\n",
    "    for frame_id, frame in enumerate(frame_results):\n",
    "        print(\"Analysing frame \", frame_id)\n",
    "        print(\"Current length trackers\", len(trackers))\n",
    "        \n",
    "        objs = frame['objects']\n",
    "        print(\"Number of obects: \", len(objs))\n",
    "        if not objs:\n",
    "            continue\n",
    "\n",
    "        if not trackers:\n",
    "            # First frame: create one tracker per object\n",
    "            for obj in objs:\n",
    "                tracker = ObjectTracker(config)\n",
    "                tracker.set_target(obj['features'], obj['bbox_3d'], obj['centroid'])\n",
    "                trackers.append(tracker)\n",
    "        else:\n",
    "            for obj in objs:\n",
    "                tracker = ObjectTracker(config)\n",
    "                tracker.set_target(obj['features'], obj['bbox_3d'], obj['centroid'])\n",
    "                \n",
    "                match = tracker.find_best_match(trackers)\n",
    "                \n",
    "                if match:\n",
    "                    match_id = match['index']\n",
    "                    trackers[match_id].update_target(obj['features'], obj['bbox_3d'], match['score'])\n",
    "                    \n",
    "                else:\n",
    "                    # No match -> create new tracker\n",
    "                    trackers.append(tracker)\n",
    "    print(\"Final length trackers\", len(trackers))\n",
    "    return trackers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7f66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
