{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3bdea2",
   "metadata": {},
   "source": [
    "# Level A Instructions:\n",
    "\n",
    "### Some steps to get familiar with the system:\n",
    "\n",
    "- Have a look at the rgb images in the folder \"ARKitScenesData/47333473/47333473_frames/lowres_wide/\" to get an impression of the scene that we are working with\n",
    "- Run the notebook which will show example detection, full pipeline results, and interactive querying\n",
    "\n",
    "## Level A\n",
    "\n",
    "This lab demonstrates open-vocabulary 3D semantic mapping using SAM (Segment Anything Model) and CLIP (Contrastive Language-Image Pre-training) with RGB-D data from the ARKitScenes dataset.\n",
    "Level A creates a semantic voxel grid where each voxel stores CLIP features, enabling text-based queries to find objects in 3D space without predefined classes.\n",
    "\n",
    "## Requirements for passing Level A:\n",
    "- Functional pipeline with semantic voxel grid construction\n",
    "- Successful text-based queries returning reasonable results\n",
    "- Understanding and explanation of rough code flow and steps required in pipeline execution\n",
    "\n",
    "## Tips:\n",
    "- In this level there is not much parameter tuning required, making the system functional by filling in code blocks and understanding/explaining the steps suffices to pass\n",
    "- There is one codeblock where you are asked to reimplement a small portion using a more efficient approach by avoiding a for loop. There is a check to make sure that outputs are equivalent that should be passed.\n",
    "- Given the somewhat higher computational expense to run the level A pipeline, by default a maximum of 40 images are processed here. Showing results for this portion is enough (see example images).\n",
    "- You may consider to lower the max_frames parameter during development to a number < 40 for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5f401",
   "metadata": {},
   "source": [
    "## 1. Dependencies and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d527f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.model_loaders import load_sam_model, load_clip_model\n",
    "from lab_utils.level_specific_viz import visualize_level_a_example, query_and_visualize_semantic_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eea080",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Level A Configuration\n",
    "    LEVEL_A_CONFIG = {\n",
    "        'frame_skip': 7,                                  \n",
    "        'max_frames': 40,                                  \n",
    "        'voxel_size': 0.1,                                 \n",
    "        'grid_size': 6,                                    \n",
    "        'sam_confidence_threshold': 0.5,                   \n",
    "        'clip_model': 'openai/clip-vit-base-patch32',      \n",
    "        'example_viz_index': 29,                          \n",
    "        'padding_ratio_image_crops': 0.1                   \n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration (for visualization)\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'depth_scale': 1000.0,\n",
    "        'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04,\n",
    "        'batch_size': 20,\n",
    "        'max_frames': 1000,\n",
    "        'volume_length': 30.0,\n",
    "        'resolution': 512,\n",
    "    }\n",
    "    \n",
    "    # Rerun Viewer Dimensions\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 700\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d2ce6",
   "metadata": {},
   "source": [
    "## 3. Execution Functions\n",
    "\n",
    "These functions orchestrate the different parts of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example_visualization(config: Config) -> Dict:\n",
    "    \"\"\"Execute example visualization.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXAMPLE VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    example_results = visualize_level_a_example(\n",
    "        config,\n",
    "        frame_index=config.LEVEL_A_CONFIG['example_viz_index']\n",
    "    )\n",
    "    \n",
    "    print(\"Example visualization complete!\")\n",
    "    return example_results\n",
    "\n",
    "\n",
    "def run_full_pipeline(config: Config) -> Dict:\n",
    "    \"\"\"Execute the complete semantic voxel mapping pipeline.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL PIPELINE EXECUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    level_a_results = run_level_a_pipeline(\n",
    "        config,\n",
    "        max_frames=config.LEVEL_A_CONFIG['max_frames'],\n",
    "        voxel_size=config.LEVEL_A_CONFIG['voxel_size'],\n",
    "        grid_size=config.LEVEL_A_CONFIG['grid_size'],\n",
    "        frame_skip_level_a=config.LEVEL_A_CONFIG['frame_skip']\n",
    "    )\n",
    "    \n",
    "    if level_a_results and 'voxel_grid' in level_a_results:\n",
    "        print(f\"\\nSemantic voxel grid created with {len(level_a_results['voxel_grid'].voxel_features)} occupied voxels\")\n",
    "        \n",
    "        # Load TSDF for visualization reference\n",
    "        print(\"Loading TSDF reference point cloud...\")\n",
    "        environment_pcd = build_tsdf_point_cloud(\n",
    "            config,\n",
    "            max_frames_for_mapping=596,\n",
    "            use_cached=True\n",
    "        )\n",
    "        \n",
    "        level_a_results['environment_pcd'] = environment_pcd\n",
    "        print(\"Full pipeline complete!\")\n",
    "    else:\n",
    "        print(\"Pipeline did not produce results. Check configuration and data.\")\n",
    "    \n",
    "    return level_a_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9f59d",
   "metadata": {},
   "source": [
    "## 4. SAM Proposal Generation\n",
    "Implement the TODOs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sam_proposals(image: Image.Image,\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          grid_size: int = 6,\n",
    "                          confidence_threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Generate object proposals using SAM with a grid of point prompts.\"\"\"\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Generate grid of point prompts\n",
    "    x_points = np.linspace(width * 0.1, width * 0.9, grid_size)\n",
    "    y_points = np.linspace(height * 0.1, height * 0.9, grid_size)\n",
    "    \n",
    "    proposals = []\n",
    "    processed_masks = []\n",
    "    \n",
    "    tqdm.write(f\"Generating SAM proposals with {grid_size}x{grid_size} grid...\")\n",
    "    \n",
    "    for i, x in enumerate(x_points):\n",
    "        for j, y in enumerate(y_points):\n",
    "            input_points = [[[x, y]]]\n",
    "            \n",
    "            try:\n",
    "                inputs = sam_processor(\n",
    "                    images=image,\n",
    "                    input_points=input_points,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = sam_model(**inputs)\n",
    "                \n",
    "                masks = sam_processor.image_processor.post_process_masks(\n",
    "                    outputs.pred_masks.cpu(),\n",
    "                    inputs[\"original_sizes\"].cpu(),\n",
    "                    inputs[\"reshaped_input_sizes\"].cpu()\n",
    "                )\n",
    "                \n",
    "                batch_masks = masks[0]\n",
    "                if len(batch_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                point_masks = batch_masks[0]\n",
    "                if len(point_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ============================================================================\n",
    "                # TODO 3: Select the best mask based on IoU/confidence scores\n",
    "                # SAM returns multiple mask predictions for each point prompt.\n",
    "                # Your task is to select the best one based on the IoU scores.\n",
    "                #\n",
    "                # The IoU scores indicate SAM's confidence in each mask.\n",
    "                # Higher IoU score = better mask quality.\n",
    "                #\n",
    "                # Steps:\n",
    "                # 1. Check if IoU scores are available in outputs\n",
    "                # 2. Extract the scores for this point's masks\n",
    "                # 3. Find the index of the highest scoring mask\n",
    "                # 4. Get that mask's confidence score\n",
    "                # 5. Skip if below threshold\n",
    "                # ============================================================================\n",
    "                \n",
    "                best_mask_idx = 0  # Default to first mask\n",
    "                best_score = 0.5   # Default confidence\n",
    "                \n",
    "                # TODO: Check if IoU scores exist and select best mask\n",
    "                # Hint: outputs.iou_scores shape is [batch_size, num_points, num_masks]\n",
    "                # We want scores for batch 0, point 0\n",
    "                \n",
    "                # if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "                #     try:\n",
    "                #         iou_scores = ???  # TODO: Extract scores for [0][0]\n",
    "                #         if len(iou_scores) > 0:\n",
    "                #             best_mask_idx = ???  # TODO: Find index of max score\n",
    "                #             best_score = ???  # TODO: Get the actual score value\n",
    "                #             \n",
    "                #             if best_score < confidence_threshold:\n",
    "                #                 continue\n",
    "                #     except:\n",
    "                #         pass\n",
    "                \n",
    "                # ============================================================================\n",
    "                # END OF TODO 3\n",
    "                # ============================================================================\n",
    "                \n",
    "                mask = point_masks[best_mask_idx]\n",
    "                if isinstance(mask, torch.Tensor):\n",
    "                    mask_np = mask.cpu().numpy().astype(bool)\n",
    "                else:\n",
    "                    mask_np = np.array(mask).astype(bool)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                is_duplicate = False\n",
    "                for existing_mask in processed_masks:\n",
    "                    overlap = np.sum(mask_np & existing_mask)\n",
    "                    union = np.sum(mask_np | existing_mask)\n",
    "                    if union > 0 and overlap / union > 0.8:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate and np.sum(mask_np) > 100:\n",
    "                    proposals.append({\n",
    "                        'mask': mask_np,\n",
    "                        'area': np.sum(mask_np),\n",
    "                        'point': [x, y],\n",
    "                        'confidence': best_score\n",
    "                    })\n",
    "                    processed_masks.append(mask_np)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    tqdm.write(f\"Generated {len(proposals)} unique segment proposals\")\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962dda6c",
   "metadata": {},
   "source": [
    "## 5. CLIP Feature Extraction\n",
    "Implement the TODOs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8282619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clip_features_from_segment(image: Image.Image,\n",
    "                                  mask: np.ndarray,\n",
    "                                  clip_model,\n",
    "                                  clip_processor,\n",
    "                                  device: str,\n",
    "                                  padding_ratio: float = 0.1) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract CLIP features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        # TODO 1: Extract bounding box and crop image for CLIP processing\n",
    "        # The mask contains True values where the object is located\n",
    "        # Find the min/max coordinates to create a bounding box\n",
    "        # Expand the box slightly using padding_ratio for better context\n",
    "        # Then crop the original image to this bounding box region\n",
    "        \n",
    "        # Hint: Use coords[0] for y-coordinates and coords[1] for x-coordinates\n",
    "        # Hint: Use image.crop((x_min, y_min, x_max, y_max)) to extract the region\n",
    "        \n",
    "        # y_min, y_max = \n",
    "        # x_min, x_max = \n",
    "        \n",
    "        # Your implementation here for expanding bounds and cropping\n",
    "        image_crop = image  # Replace this line\n",
    "        \n",
    "        # Process with CLIP\n",
    "        inputs = clip_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "            # TODO 2: Normalize CLIP features using L2 normalization\n",
    "            # CLIP features work best when normalized to unit length\n",
    "            # This ensures that similarity comparisons focus on direction, not magnitude\n",
    "            \n",
    "            # Your implementation here\n",
    "            pass  # Remove this and add normalization\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract CLIP features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0pqn335h0d7e",
   "metadata": {},
   "source": [
    "Visualize the SAM+CLIP process on a single frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "of414sv125",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_results = visualize_level_a_example(\n",
    "    config,\n",
    "    frame_index=config.LEVEL_A_CONFIG['example_viz_index']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c45b8c",
   "metadata": {},
   "source": [
    "## 6. Semantic Voxel Grid\n",
    "Implement the TODOs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d55edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticVoxelGrid:\n",
    "    \"\"\"Voxel grid that stores CLIP embeddings for semantic querying.\"\"\"\n",
    "    def __init__(self, voxel_size: float = 0.1, feature_dim: int = 512):\n",
    "        \"\"\"Initialize semantic voxel grid.\"\"\"\n",
    "        self.voxel_size = voxel_size\n",
    "        self.feature_dim = feature_dim\n",
    "        self.voxel_features = {}\n",
    "        self.voxel_counts = {}\n",
    "        self.bounds_min = None\n",
    "        self.bounds_max = None\n",
    "            \n",
    "    def world_to_voxel(self, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert world coordinates to voxel indices.\"\"\"\n",
    "        return np.floor(points / self.voxel_size).astype(int)\n",
    "\n",
    "    def voxel_to_world(self, indices: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert voxel indices to world coordinates (center of voxel).\"\"\"\n",
    "        return (indices + 0.5) * self.voxel_size\n",
    "\n",
    "    def add_semantic_points(self, points: np.ndarray, features: np.ndarray):\n",
    "        \"\"\"Add points with semantic features to the voxel grid.\"\"\"\n",
    "        if len(points) == 0:\n",
    "            return\n",
    "        \n",
    "        # Update bounds\n",
    "        if self.bounds_min is None:\n",
    "            self.bounds_min = np.min(points, axis=0)\n",
    "            self.bounds_max = np.max(points, axis=0)\n",
    "        else:\n",
    "            self.bounds_min = np.minimum(self.bounds_min, np.min(points, axis=0))\n",
    "            self.bounds_max = np.maximum(self.bounds_max, np.max(points, axis=0))\n",
    "        \n",
    "        # Convert to voxel indices\n",
    "        voxel_indices = self.world_to_voxel(points)\n",
    "        \n",
    "        # Group points by voxel\n",
    "        unique_voxels = np.unique(voxel_indices, axis=0)\n",
    "        \n",
    "        for voxel_idx in unique_voxels:\n",
    "            voxel_key = tuple(voxel_idx)\n",
    "            \n",
    "            if voxel_key not in self.voxel_features:\n",
    "                self.voxel_features[voxel_key] = np.zeros(self.feature_dim)\n",
    "                self.voxel_counts[voxel_key] = 0\n",
    "            \n",
    "            # Running average of features\n",
    "            self.voxel_features[voxel_key] += features\n",
    "            self.voxel_counts[voxel_key] += 1\n",
    "\n",
    "    def get_normalized_features(self) -> Dict:\n",
    "        \"\"\"Get normalized average features for each voxel.\"\"\"\n",
    "        normalized = {}\n",
    "        for voxel_key, features in self.voxel_features.items():\n",
    "            count = self.voxel_counts[voxel_key]\n",
    "            if count > 0:\n",
    "                avg_features = features / count\n",
    "                avg_features = avg_features / (np.linalg.norm(avg_features) + 1e-6)\n",
    "                normalized[voxel_key] = avg_features\n",
    "        return normalized\n",
    "\n",
    "    def query_text(self, text_query: str, clip_model, clip_processor, device: str) -> tuple:\n",
    "        \"\"\"Query the voxel grid with text and return similarity scores.\"\"\"\n",
    "        # Encode text query\n",
    "        inputs = clip_processor(text=[text_query], return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**inputs)\n",
    "\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        text_features = text_features.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Compute similarities\n",
    "        normalized_features = self.get_normalized_features()\n",
    "        \n",
    "        if not normalized_features:\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        # ============================================================================\n",
    "        # NON-VECTORIZED IMPLEMENTATION (PROVIDED AS REFERENCE)\n",
    "        # This implementation uses a for loop to compute cosine similarities\n",
    "        # ============================================================================\n",
    "        voxel_centers_loop = []\n",
    "        similarities_loop = []\n",
    "        \n",
    "        for voxel_key, voxel_features in normalized_features.items():\n",
    "            # Compute cosine similarity using dot product (features are already normalized)\n",
    "            similarity = np.dot(text_features, voxel_features)\n",
    "            voxel_center = self.voxel_to_world(np.array(voxel_key))\n",
    "            voxel_centers_loop.append(voxel_center)\n",
    "            similarities_loop.append(similarity)\n",
    "        \n",
    "        voxel_centers_loop = np.array(voxel_centers_loop)\n",
    "        similarities_loop = np.array(similarities_loop)\n",
    "        \n",
    "        # ============================================================================\n",
    "        # TODO 1: VECTORIZED IMPLEMENTATION\n",
    "        # Task: Implement a vectorized version that avoids the for loop above.\n",
    "        # \n",
    "        # Hints:\n",
    "        # 1. Extract all voxel keys and features into numpy arrays\n",
    "        # 2. Use matrix multiplication (@) for computing all similarities at once\n",
    "        # 3. Convert voxel indices to world coordinates in a vectorized manner\n",
    "        #\n",
    "        # Your implementation should compute:\n",
    "        # - voxel_centers_vectorized: numpy array of shape (num_voxels, 3)\n",
    "        # - similarities_vectorized: numpy array of shape (num_voxels,)\n",
    "        # ============================================================================\n",
    "        \n",
    "        # Student implementation goes here\n",
    "        voxel_keys = None  # Extract keys from normalized_features\n",
    "        voxel_features_matrix = None  # Stack all feature vectors into a matrix\n",
    "        voxel_indices = None  # Convert keys to numpy array\n",
    "        \n",
    "        # Compute all similarities at once using matrix multiplication\n",
    "        similarities_vectorized = None\n",
    "        \n",
    "        # Transform all voxel indices to world coordinates at once\n",
    "        voxel_centers_vectorized = None  # Use self.voxel_to_world()\n",
    "        \n",
    "        # ============================================================================\n",
    "        # VERIFICATION: Check if both implementations produce the same results\n",
    "        # ============================================================================\n",
    "        if voxel_centers_vectorized is not None and similarities_vectorized is not None:\n",
    "            # Check if the results match (allowing for small numerical differences)\n",
    "            centers_match = np.allclose(voxel_centers_loop, voxel_centers_vectorized, rtol=1e-5)\n",
    "            similarities_match = np.allclose(similarities_loop, similarities_vectorized, rtol=1e-5)\n",
    "            \n",
    "            if centers_match and similarities_match:\n",
    "                print(\"✓ Success! Vectorized implementation produces identical results.\")\n",
    "                print(f\"  Processed {len(similarities_vectorized)} voxels\")\n",
    "            else:\n",
    "                print(\"✗ Mismatch detected between implementations:\")\n",
    "                if not centers_match:\n",
    "                    print(\"  - Voxel centers don't match\")\n",
    "                    print(f\"    Max difference: {np.max(np.abs(voxel_centers_loop - voxel_centers_vectorized))}\")\n",
    "                if not similarities_match:\n",
    "                    print(\"  - Similarities don't match\")\n",
    "                    print(f\"    Max difference: {np.max(np.abs(similarities_loop - similarities_vectorized))}\")\n",
    "            \n",
    "            # Return the vectorized version if available, otherwise fall back to loop version\n",
    "            return voxel_centers_vectorized, similarities_vectorized\n",
    "        else:\n",
    "            print(\"⚠ Vectorized implementation not complete, using loop version\")\n",
    "            return voxel_centers_loop, similarities_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476269f4",
   "metadata": {},
   "source": [
    "## 7. 3D Point Cloud Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_to_3d_pointcloud(mask: np.ndarray,\n",
    "                             depth_image: np.ndarray,\n",
    "                             rgb_image: np.ndarray,\n",
    "                             camera_intrinsics: np.ndarray,\n",
    "                             camera_pose: np.ndarray,\n",
    "                             depth_scale: float = 1000.0,\n",
    "                             max_points: int = 5000,\n",
    "                             min_depth: float = 0.1,\n",
    "                             max_depth: float = 10.0) -> Optional[Dict]:\n",
    "    \"\"\"Convert segmented region to 3D point cloud.\"\"\"\n",
    "    # Get mask indices\n",
    "    mask_indices = np.where(mask)\n",
    "    \n",
    "    if len(mask_indices[0]) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Extract depth values for masked region\n",
    "    depths = depth_image[mask_indices] / depth_scale\n",
    "    \n",
    "    # Filter valid depths\n",
    "    valid_mask = (depths > min_depth) & (depths < max_depth)\n",
    "    if not np.any(valid_mask):\n",
    "        return None\n",
    "    \n",
    "    # Get valid coordinates and depths\n",
    "    v_coords = mask_indices[0][valid_mask]\n",
    "    u_coords = mask_indices[1][valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    \n",
    "    # Subsample if too many points\n",
    "    num_points = len(v_coords)\n",
    "    if num_points > max_points:\n",
    "        indices = np.random.choice(num_points, max_points, replace=False)\n",
    "        v_coords = v_coords[indices]\n",
    "        u_coords = u_coords[indices]\n",
    "        valid_depths = valid_depths[indices]\n",
    "    \n",
    "    # Get camera parameters\n",
    "    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
    "    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
    "    \n",
    "    # Project to 3D camera coordinates\n",
    "    x_cam = (u_coords - cx) * valid_depths / fx\n",
    "    y_cam = (v_coords - cy) * valid_depths / fy\n",
    "    z_cam = valid_depths\n",
    "    \n",
    "    # Stack into points\n",
    "    points_cam = np.stack([x_cam, y_cam, z_cam], axis=-1)\n",
    "    \n",
    "    # Transform to world coordinates\n",
    "    points_cam_hom = np.concatenate([points_cam, np.ones((len(points_cam), 1))], axis=1)\n",
    "    camera_pose_inv = np.linalg.inv(camera_pose)\n",
    "    points_world_hom = (camera_pose_inv @ points_cam_hom.T).T\n",
    "    points_world = points_world_hom[:, :3]\n",
    "    \n",
    "    # Get colors if RGB image provided\n",
    "    colors = None\n",
    "    if rgb_image is not None:\n",
    "        colors = rgb_image[v_coords, u_coords]\n",
    "        if colors.dtype != np.uint8:\n",
    "            colors = (colors * 255).astype(np.uint8)\n",
    "    \n",
    "    return {\n",
    "        'points': points_world,\n",
    "        'colors': colors,\n",
    "        'camera_points': points_cam,\n",
    "        'num_points': len(points_world)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668d0e0",
   "metadata": {},
   "source": [
    "## 8. Frame Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688248ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_level_a(frame_data: Dict,\n",
    "        sam_model, sam_processor,\n",
    "        clip_model, clip_processor,\n",
    "        device: str,\n",
    "        config: Config,\n",
    "        grid_size: int = 6) -> Dict:\n",
    "    \"\"\"Process a single frame for Level A: SAM proposals + CLIP features.\"\"\"\n",
    "    results = {\n",
    "    'frame_name': frame_data['frame_name'],\n",
    "    'segments': [],\n",
    "    'semantic_points': []\n",
    "    }\n",
    "    try:\n",
    "        # Load images\n",
    "        image = Image.open(frame_data['rgb_path']).convert(\"RGB\")\n",
    "        depth_image = cv2.imread(frame_data['depth_path'], cv2.IMREAD_UNCHANGED)\n",
    "        rgb_image = np.array(image)\n",
    "        \n",
    "        # Generate SAM proposals\n",
    "        proposals = generate_sam_proposals(\n",
    "            image,\n",
    "            sam_model,\n",
    "            sam_processor,\n",
    "            device,\n",
    "            grid_size=grid_size,\n",
    "            confidence_threshold=config.LEVEL_A_CONFIG['sam_confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        # Process each proposal\n",
    "        for proposal in proposals:\n",
    "            mask = proposal['mask']\n",
    "            \n",
    "            # Extract CLIP features\n",
    "            clip_features = extract_clip_features_from_segment(\n",
    "                image,\n",
    "                mask,\n",
    "                clip_model,\n",
    "                clip_processor,\n",
    "                device,\n",
    "                padding_ratio=config.LEVEL_A_CONFIG['padding_ratio_image_crops'] \n",
    "            )\n",
    "            \n",
    "            if clip_features is None:\n",
    "                continue\n",
    "            \n",
    "            # Convert to 3D points\n",
    "            pc_data = segment_to_3d_pointcloud(\n",
    "                mask,\n",
    "                depth_image,\n",
    "                rgb_image,\n",
    "                frame_data['camera_intrinsics'],\n",
    "                frame_data['camera_pose'],\n",
    "                depth_scale=config.TSDF_CONFIG['depth_scale'],\n",
    "                max_points=1000\n",
    "            )\n",
    "            \n",
    "            if pc_data is None or pc_data['num_points'] < 10:\n",
    "                continue\n",
    "            \n",
    "            results['segments'].append({\n",
    "                'mask': mask,\n",
    "                'area': proposal['area'],\n",
    "                'confidence': proposal['confidence']\n",
    "            })\n",
    "            \n",
    "            results['semantic_points'].append({\n",
    "                'points': pc_data['points'],\n",
    "                'features': clip_features,\n",
    "                'frame_name': frame_data['frame_name']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_data['frame_name']}: {e}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d4240",
   "metadata": {},
   "source": [
    "## 9. Full Pipeline Execution\n",
    "\n",
    "Now let's run the complete semantic voxel mapping pipeline across all frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_level_a_pipeline(config: Config,\n",
    "                        max_frames: int = 35,\n",
    "                        voxel_size: float = 0.1,\n",
    "                        grid_size: int = 6,\n",
    "                        frame_skip_level_a: int = 7) -> Dict:\n",
    "    \"\"\"Run Level A pipeline: SAM + CLIP for semantic voxel mapping.\"\"\"\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    sam_model, sam_processor, device = load_sam_model(model_size='base')\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "    \n",
    "    # Get CLIP feature dimension\n",
    "    dummy_input = clip_processor(images=Image.new('RGB', (224, 224)), return_tensors=\"pt\")\n",
    "    dummy_input = {k: v.to(device) for k, v in dummy_input.items()}\n",
    "    with torch.no_grad():\n",
    "        dummy_features = clip_model.get_image_features(**dummy_input)\n",
    "    feature_dim = dummy_features.shape[-1]\n",
    "    print(f\"CLIP feature dimension: {feature_dim}\")\n",
    "    \n",
    "    # Load camera data\n",
    "    camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "    \n",
    "    # Get frames with Level A specific frame skip\n",
    "    frames_metadata = get_frame_list(config.RGB_PATH, frame_skip_level_a)\n",
    "    print(f\"Selected {len(frames_metadata)} frames with frame skip of {frame_skip_level_a}\")\n",
    "    \n",
    "    # Align frames\n",
    "    aligned_frames = validate_and_align_frame_data(\n",
    "        frames_metadata,\n",
    "        camera_poses,\n",
    "        config.RGB_PATH,\n",
    "        config.DEPTH_PATH,\n",
    "        config.INTRINSICS_PATH\n",
    "    )\n",
    "    \n",
    "    if not aligned_frames:\n",
    "        print(\"No aligned frames found!\")\n",
    "        return {}\n",
    "    \n",
    "    # Create semantic voxel grid\n",
    "    voxel_grid = SemanticVoxelGrid(voxel_size=voxel_size, feature_dim=feature_dim)\n",
    "    \n",
    "    # Process frames\n",
    "    frames_to_process = aligned_frames[:max_frames]\n",
    "    print(f\"Processing {len(frames_to_process)} frames...\")\n",
    "    \n",
    "    for frame_data in tqdm(frames_to_process, desc=\"Processing frames\"):\n",
    "        frame_results = process_frame_level_a(\n",
    "            frame_data,\n",
    "            sam_model, sam_processor,\n",
    "            clip_model, clip_processor,\n",
    "            device,\n",
    "            config,\n",
    "            grid_size=grid_size\n",
    "        )\n",
    "        \n",
    "        # Add semantic points to voxel grid\n",
    "        for semantic_data in frame_results['semantic_points']:\n",
    "            voxel_grid.add_semantic_points(\n",
    "                semantic_data['points'],\n",
    "                semantic_data['features']\n",
    "            )\n",
    "    \n",
    "    print(f\"\\nLevel A Results:\")\n",
    "    print(f\"  Occupied voxels: {len(voxel_grid.voxel_features)}\")\n",
    "    \n",
    "    return {\n",
    "        'voxel_grid': voxel_grid,\n",
    "        'clip_model': clip_model,\n",
    "        'clip_processor': clip_processor,\n",
    "        'device': device\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aab80b",
   "metadata": {},
   "source": [
    "## 10. Complete pipeline\n",
    "\n",
    "This notebook demonstrates the complete Level A pipeline: SAM + CLIP semantic voxel mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete semantic voxel mapping pipeline\n",
    "print(\"Running full SAM + CLIP semantic mapping pipeline...\")\n",
    "\n",
    "pipeline_results = run_full_pipeline(config)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LEVEL A EXECUTION COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea89b1",
   "metadata": {},
   "source": [
    "## 11. Interactive Semantic Querying\n",
    "\n",
    "Now you can query the semantic voxel grid with natural language! Try different queries to find objects in 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive querying - you can modify the query and run this cell multiple times\n",
    "if 'pipeline_results' in locals() and pipeline_results and 'voxel_grid' in pipeline_results:\n",
    "    # ============================================\n",
    "    # MODIFY THIS QUERY TO SEARCH FOR DIFFERENT OBJECTS\n",
    "    # ============================================\n",
    "    query = \"a pillow\"  # Try: \"bed\", \"pillow\", \"shelf\", \"table\", \"sofa\", \"chair\", etc.\n",
    "                       # You can also try abstract prompts like \"a place to sit\"\n",
    "    \n",
    "    # Use the environment point cloud from pipeline results\n",
    "    environment_pcd = pipeline_results.get('environment_pcd')\n",
    "    \n",
    "    print(f\"Querying semantic voxel grid for: '{query}'\")\n",
    "    query_and_visualize_semantic_grid(\n",
    "        pipeline_results,\n",
    "        query,\n",
    "        environment_pcd,\n",
    "        config\n",
    "    )\n",
    "else:\n",
    "    print(\"Pipeline results not available. Run the pipeline execution cell above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
