{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e99834",
   "metadata": {},
   "source": [
    "# Project part E - Refactored with ObjectTracker Class\n",
    "\n",
    "Static object tracking through multiple frames using SAM and DINOv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6299586",
   "metadata": {},
   "source": [
    "## Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import open3d as o3d\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import AutoImageProcessor, AutoModel, pipeline\n",
    "import requests\n",
    "import imageio\n",
    "\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data, load_camera_intrinsics\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.model_loaders import load_sam_model, load_clip_model, load_dino_model\n",
    "from lab_utils.visualization_utils import visualize_sam_proposals_grid\n",
    "from lab_utils.viz_eval import visualize_3d_scene_bbox_results\n",
    "from lab_utils.ground_truth import load_ground_truth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b9cfd",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90994386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Project Configuration\n",
    "    PROJECT_CONFIG = {\n",
    "        'frame_skip': 7,                                  \n",
    "        'max_frames': 40,                                  \n",
    "        'grid_size': 6,                                    \n",
    "        'sam_confidence_threshold': 0.5,                   \n",
    "        'clip_model': 'openai/clip-vit-base-patch32',      \n",
    "        'example_viz_index': 29,                          \n",
    "        'padding_ratio_image_crops': 0.1                   \n",
    "    }\n",
    "    \n",
    "    # Tracking Configuration (NEW)\n",
    "    TRACKING_CONFIG = {\n",
    "        'similarity_threshold': 0.6,      # Minimum score to consider a match\n",
    "        'feature_weight': 0.8,            # Weight for feature similarity (alpha)\n",
    "        'spatial_weight': 0.2,            # Weight for spatial similarity (1-alpha)\n",
    "        'moving_target': False,           # Enable target updates\n",
    "        'update_momentum': 0.1,           # How much to update target (0.1 = conservative)\n",
    "        'confidence_threshold': 0.8,      # Minimum confidence for target update\n",
    "        'use_depth': True                 # Use depth from RGB-D\n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration (for visualization)\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'depth_scale': 1000.0,\n",
    "        'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04,\n",
    "        'batch_size': 20,\n",
    "        'max_frames': 1000,\n",
    "        'volume_length': 30.0,\n",
    "        'resolution': 512,\n",
    "    }\n",
    "\n",
    "    # GT and Display Configuration\n",
    "    GT_CONFIG = {\n",
    "        'allowed_classes': None, 'mesh_downsample_points': 75000,\n",
    "        'show_mesh': True, 'show_annotations': True\n",
    "    }\n",
    "    \n",
    "    # Rerun Viewer Dimensions\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 700\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7ae9e",
   "metadata": {},
   "source": [
    "## Core Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867070ea",
   "metadata": {},
   "source": [
    "### Object Tracker Class\n",
    "\n",
    "The `ObjectTracker` class encapsulates all the tracking logic in a clean interface. It handles:\n",
    "\n",
    "- **Target Management**: Setting and updating the reference object features\n",
    "- **Configuration**: Using the centralized tracking config for all parameters\n",
    "- **Similarity Matching**: Combining DINOv2 features with 3D spatial information  \n",
    "- **Moving Target Updates**: Optionally adapting to appearance changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    \"\"\"Lightweight object tracker for multi-frame tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.target_features = None\n",
    "        self.target_bbox = None\n",
    "        self.target_centroid = None\n",
    "        self.clip_features = []\n",
    "        self.tracking_history = []  # Optional: for debugging/analysis\n",
    "        \n",
    "    def set_target(self, features, bbox_3d, centroid, clip_features = None):\n",
    "        \"\"\"Set the initial target to track.\"\"\"\n",
    "        self.target_features = features.copy() if hasattr(features, 'copy') else np.array(features)\n",
    "        self.target_bbox = bbox_3d.copy() if hasattr(bbox_3d, 'copy') else np.array(bbox_3d)\n",
    "        self.target_centroid = centroid.copy() if hasattr(centroid, 'copy') else np.array(centroid)\n",
    "        if clip_features is not None:\n",
    "            self.clip_features.append(clip_features)\n",
    "        \n",
    "    def find_best_match(self, objects):\n",
    "        \"\"\"Find the best matching object in the current frame.\"\"\"\n",
    "        if self.target_features is None:\n",
    "            return None\n",
    "            \n",
    "        if not objects:\n",
    "            return None\n",
    "            \n",
    "        best_idx = None\n",
    "        best_score = 0.0\n",
    "        best_sim_features = 0.0  \n",
    "        best_sim_spatial = 0.0\n",
    "        similarity_threshold = self.config.TRACKING_CONFIG['similarity_threshold']\n",
    "        alpha = self.config.TRACKING_CONFIG['feature_weight']\n",
    "\n",
    "        for i, obj in enumerate(objects):\n",
    "\n",
    "            if isinstance(obj, ObjectTracker):\n",
    "                # Object is another tracker\n",
    "                features = obj.target_features\n",
    "                bbox_3d = obj.target_bbox\n",
    "                centroid = obj.target_centroid\n",
    "            else:\n",
    "                # Candidate is a dict\n",
    "                features = obj['features']\n",
    "                bbox_3d = obj['bbox_3d']\n",
    "                centroid = obj['centroid']\n",
    "            \n",
    "            # Hard distance cutoff \n",
    "            target_centroid = (self.target_bbox[0] + self.target_bbox[1]) / 2.0\n",
    "            distance = np.linalg.norm(centroid - target_centroid)\n",
    "            if distance > 0.5:  # Hard threshold\n",
    "                continue\n",
    "            \n",
    "            # Feature similarity (cosine similarity)\n",
    "            sim_features = np.dot(self.target_features, features) / (\n",
    "                np.linalg.norm(self.target_features) * np.linalg.norm(features) + 1e-8\n",
    "            )\n",
    "            \n",
    "            # Spatial similarity (3D IoU)\n",
    "            sim_spatial = self._bbox3d_iou(self.target_bbox, bbox_3d)\n",
    "\n",
    "            # Combined score\n",
    "            score = alpha * sim_features + (1 - alpha) * sim_spatial\n",
    "            \n",
    "            if score > best_score and score >= similarity_threshold:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "                best_sim_features = sim_features  \n",
    "                best_sim_spatial = sim_spatial   \n",
    "                \n",
    "        if best_idx is not None:\n",
    "            match_result = {\n",
    "                'index': best_idx, \n",
    "                'score': best_score,\n",
    "                'feature_sim': best_sim_features,\n",
    "                'spatial_sim': best_sim_spatial\n",
    "            }\n",
    "            #self.tracking_history.append(match_result)\n",
    "            return match_result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def update_target(self, new_features, new_bbox, confidence_score, new_clip_features = None):\n",
    "        \"\"\"Update target with new features (moving target).\"\"\"\n",
    "\n",
    "        self.tracking_history.append([self.target_features, self.target_bbox])\n",
    "        if not self.config.TRACKING_CONFIG['moving_target']:\n",
    "            return  # Moving target disabled\n",
    "            \n",
    "        confidence_threshold = self.config.TRACKING_CONFIG['confidence_threshold']\n",
    "        if confidence_score < confidence_threshold:\n",
    "            return  # Not confident enough to update        \n",
    "            \n",
    "        momentum = self.config.TRACKING_CONFIG['update_momentum']\n",
    "        \n",
    "        # Weighted update - keeps some \"memory\" of original target\n",
    "        self.target_features = (1 - momentum) * self.target_features + momentum * new_features\n",
    "        self.target_bbox = (1 - momentum) * self.target_bbox + momentum * new_bbox\n",
    "        if new_clip_features is not None:\n",
    "            self.clip_features.append(new_clip_features)\n",
    "        \n",
    "\n",
    "    \n",
    "    def _bbox3d_iou(self, b1, b2):\n",
    "        \"\"\"Calculate 3D IoU between two bounding boxes.\"\"\"\n",
    "        b1_min, b1_max = b1[0], b1[1]\n",
    "        b2_min, b2_max = b2[0], b2[1]\n",
    "\n",
    "        x_min = max(b1_min[0], b2_min[0])\n",
    "        y_min = max(b1_min[1], b2_min[1])\n",
    "        z_min = max(b1_min[2], b2_min[2])\n",
    "        x_max = min(b1_max[0], b2_max[0])\n",
    "        y_max = min(b1_max[1], b2_max[1])\n",
    "        z_max = min(b1_max[2], b2_max[2])\n",
    "        \n",
    "        if x_min >= x_max or y_min >= y_max or z_min >= z_max:\n",
    "            return 0.0\n",
    "\n",
    "        inter_vol = (x_max - x_min) * (y_max - y_min) * (z_max - z_min)\n",
    "        vol1 = (b1_max - b1_min).prod()\n",
    "        vol2 = (b2_max - b2_min).prod()\n",
    "        \n",
    "        return inter_vol / (vol1 + vol2 - inter_vol + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9ac54",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8476f",
   "metadata": {},
   "source": [
    "### SAM Proposals\n",
    "\n",
    "We filter proposals by confidence threshold and remove duplicates to get clean object candidates.\n",
    "\n",
    "Here we use SAM (Segment Anything Model) with a grid of point prompts, same approach as in Lab2 to generate object proposals.\n",
    "\n",
    "Each proposal contains:\n",
    "- **point**: Grid point that generated this proposal\n",
    "- **confidence**: SAM's confidence score for this segmentation\n",
    "- **mask**: Binary segmentation mask for the object\n",
    "- **area**: Number of pixels in the mask (for filtering small objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sam_proposals_project(image: Image.Image,\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          grid_size: int = 6,\n",
    "                          confidence_threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Generate object proposals using SAM with a grid of point prompts.\"\"\"\n",
    "    width, height = image.size\n",
    "    # Generate grid of point prompts\n",
    "    x_points = np.linspace(width * 0.1, width * 0.9, grid_size)\n",
    "    y_points = np.linspace(height * 0.1, height * 0.9, grid_size)\n",
    "    \n",
    "    proposals = []\n",
    "    processed_masks = []\n",
    "    \n",
    "    tqdm.write(f\"Generating SAM proposals with {grid_size}x{grid_size} grid...\")\n",
    "    \n",
    "    for i, x in enumerate(x_points):\n",
    "        for j, y in enumerate(y_points):\n",
    "            input_points = [[[x, y]]]\n",
    "            \n",
    "            try:\n",
    "                inputs = sam_processor(\n",
    "                    images=image,\n",
    "                    input_points=input_points,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = sam_model(**inputs)\n",
    "                \n",
    "                masks = sam_processor.image_processor.post_process_masks(\n",
    "                    outputs.pred_masks.cpu(),\n",
    "                    inputs[\"original_sizes\"].cpu(),\n",
    "                    inputs[\"reshaped_input_sizes\"].cpu()\n",
    "                )\n",
    "                \n",
    "                batch_masks = masks[0]\n",
    "                if len(batch_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                point_masks = batch_masks[0]\n",
    "                if len(point_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                best_mask_idx = 0  # Default to first mask\n",
    "                best_score = 0.5   # Default confidence\n",
    "                \n",
    "                if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "                    try:\n",
    "                        iou_scores = outputs.iou_scores[0,0,:].cpu().numpy()\n",
    "                        if len(iou_scores) > 0:\n",
    "                            best_mask_idx = int(np.argmax(iou_scores))\n",
    "                            best_score = float(iou_scores[best_mask_idx])\n",
    "                             \n",
    "                            if best_score < confidence_threshold:\n",
    "                                continue\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                mask = point_masks[best_mask_idx]\n",
    "                if isinstance(mask, torch.Tensor):\n",
    "                    mask_np = mask.cpu().numpy().astype(bool)\n",
    "                else:\n",
    "                    mask_np = np.array(mask).astype(bool)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                is_duplicate = False\n",
    "                for existing_mask in processed_masks:\n",
    "                    overlap = np.sum(mask_np & existing_mask)\n",
    "                    union = np.sum(mask_np | existing_mask)\n",
    "                    if union > 0 and overlap / union > 0.8:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate and np.sum(mask_np) > 100:\n",
    "                    proposals.append({\n",
    "                        'mask': mask_np,\n",
    "                        'area': np.sum(mask_np),\n",
    "                        'point': [x, y],\n",
    "                        'confidence': best_score\n",
    "                    })\n",
    "                    processed_masks.append(mask_np)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    tqdm.write(f\"Generated {len(proposals)} unique segment proposals\")\n",
    "\n",
    "    #visualize_sam_proposals_grid(image, proposals, x_points, y_points)\n",
    "\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021d51a",
   "metadata": {},
   "source": [
    "### CLIP Features\n",
    "\n",
    "These features are used for initial target selection when a text query is provided.\n",
    "\n",
    "We extract CLIP features from segmented regions to enable text-based object queries (like \"ball\" or \"sofa\").\n",
    "\n",
    "The process:\n",
    "1. **Make bounding box** of the mask\n",
    "2. **Crop the image** to the bounding box\n",
    "3. **Expand with padding** to include context around the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clip_features_from_segment(image: Image.Image,\n",
    "                                  mask: np.ndarray,\n",
    "                                  clip_model,\n",
    "                                  clip_processor,\n",
    "                                  device: str,\n",
    "                                  padding_ratio: float = 0.1) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract CLIP features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((int(x_min_exp), int(y_min_exp), int(x_max_exp), int(y_max_exp)))\n",
    "                \n",
    "        # Process with CLIP\n",
    "        inputs = clip_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "            \n",
    "            # Normalize image features\n",
    "            image_features = image_features/ image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract CLIP features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceda3d8",
   "metadata": {},
   "source": [
    "### DINOv2 Features\n",
    "\n",
    "\n",
    "DINOv2 provides superior visual features for object tracking compared to CLIP, especially for appearance-based matching across frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dino_features_from_segment(\n",
    "    image: Image.Image,\n",
    "    mask: np.ndarray,\n",
    "    dino_model,\n",
    "    dino_processor,\n",
    "    device: str,\n",
    "    padding_ratio: float = 0.1\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract DINOv2 features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((x_min_exp, y_min_exp, x_max_exp, y_max_exp))\n",
    "        \n",
    "        # Process with DINOv2\n",
    "        inputs = dino_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "            # Use the [CLS] token embedding or pooled output as image feature\n",
    "            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                image_features = outputs.pooler_output\n",
    "            else:\n",
    "                image_features = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract DINO features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e862f47",
   "metadata": {},
   "source": [
    "## 3D Position Extraction\n",
    "\n",
    "To enable spatial tracking in world coordinates, we convert 2D segmentation masks into 3D object representations using camera geometry and depth data.\n",
    "\n",
    "#### **What We Extract:**\n",
    "- **Object position**: 3D centroid in world coordinates  \n",
    "- **Object extent**: Axis-aligned bounding box for size/shape information\n",
    "- **Spatial context**: Used for 3D IoU calculations in tracking similarity\n",
    "\n",
    "#### **The Pipeline:**\n",
    "\n",
    "**1. Pixel Sampling**\n",
    "- **Sample up to 500 pixels** from the mask for computational efficiency\n",
    "- **Filter minimum size**: Skip masks with fewer than 20 pixels (noise rejection)\n",
    "\n",
    "**2. Depth Processing** \n",
    "- **Extract depth values** at sampled pixel locations\n",
    "- **Filter valid depths**: Remove pixels with depth ≤ 0 (invalid/missing data)\n",
    "\n",
    "**3. 3D Reconstruction**\n",
    "- **Back-project to camera space**: Use intrinsics (fx, fy, cx, cy) to convert pixels → 3D points\n",
    "- **Transform to world space**: Apply camera pose matrix for global coordinates\n",
    "\n",
    "**4. Compact Representation**\n",
    "- **Centroid**: Mean position of all valid 3D points\n",
    "- **Bounding box**: [min_xyz, max_xyz] extent for spatial similarity\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Efficient**: Sampling limits computation while preserving shape information\n",
    "- **Robust**: Filters invalid depth data that could corrupt 3D positions  \n",
    "- **Tracking-ready**: Compact format perfect for 3D IoU calculations between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_3d_position_from_mask(\n",
    "    mask: np.ndarray,\n",
    "    depth_image: np.ndarray,\n",
    "    camera_intrinsics: np.ndarray,\n",
    "    camera_pose: np.ndarray,\n",
    "    depth_scale: float,\n",
    "    max_points: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"Convert a binary mask into compact 3D object representation.\"\"\"\n",
    "    coords = np.column_stack(np.where(mask))\n",
    "    if coords.shape[0] < 20:\n",
    "        return None\n",
    "\n",
    "    if coords.shape[0] > max_points:\n",
    "        sample_idx = np.random.choice(len(coords), max_points, replace=False)\n",
    "        coords = coords[sample_idx]\n",
    "\n",
    "    z = depth_image[coords[:, 0], coords[:, 1]].astype(np.float32) / depth_scale\n",
    "    valid = z > 0\n",
    "    if not np.any(valid):\n",
    "        return None\n",
    "    coords = coords[valid]\n",
    "    z = z[valid]\n",
    "\n",
    "    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
    "    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
    "\n",
    "    # Convert to camera coordinates\n",
    "    x = (coords[:, 1] - cx) * z / fx\n",
    "    y = (coords[:, 0] - cy) * z / fy\n",
    "    pts_cam = np.stack([x, y, z], axis=-1)\n",
    "\n",
    "    # Transform to world coordinates using INVERSE (like Level A & C)\n",
    "    pts_h = np.hstack([pts_cam, np.ones((pts_cam.shape[0], 1))])\n",
    "    camera_pose_inv = np.linalg.inv(camera_pose)  # <-- THIS IS CRITICAL\n",
    "    pts_world = (camera_pose_inv @ pts_h.T).T[:, :3]\n",
    "\n",
    "    # Compute compact representation\n",
    "    centroid = pts_world.mean(axis=0)\n",
    "    bbox_min = pts_world.min(axis=0)\n",
    "    bbox_max = pts_world.max(axis=0)\n",
    "\n",
    "    return {\n",
    "        'centroid': centroid.astype(np.float32),\n",
    "        'bbox_3d': np.stack([bbox_min, bbox_max]).astype(np.float32)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09fc69",
   "metadata": {},
   "source": [
    "## Frame Processing\n",
    "\n",
    "Each processed frame contains a list of detected objects with their visual features, spatial location, and metadata.\n",
    "\n",
    "This is where we put it all together for each frame:\n",
    "\n",
    "1. **Load RGB and depth images** for the frame\n",
    "\n",
    "2. **Generate SAM proposals** using the grid-based approach\n",
    "3. **Extract features** for each proposal (both DINOv2 and CLIP)\n",
    "4. **Calculate 3D positions** from masks and depth data\n",
    "5. **Package results** into a structured format for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_project(frame_data: Dict,\n",
    "        sam_model, sam_processor,\n",
    "        dino_model, dino_processor,\n",
    "        clip_model, clip_processor,\n",
    "        device: str,\n",
    "        config: Config,\n",
    "        grid_size: int = 6) -> Dict:\n",
    "    \"\"\"Process a single frame for Project: SAM proposals + CLIP embeddings + DINOv2 features.\"\"\"\n",
    "    results = {\n",
    "        'frame_name': frame_data['frame_name'],\n",
    "        'objects': []\n",
    "    }\n",
    "    try:\n",
    "        # Load images\n",
    "        image = Image.open(frame_data['rgb_path']).convert(\"RGB\")\n",
    "        rgb_image = np.array(image)\n",
    "        \n",
    "         # Choose depth images\n",
    "        if config.TRACKING_CONFIG[\"use_depth\"]:\n",
    "            print(\"Using RGB-D for depth\")\n",
    "            depth_image = cv2.imread(frame_data['depth_path'], cv2.IMREAD_UNCHANGED)\n",
    "        else:\n",
    "            # Use an apporximation of the depth (DepthAnything)\n",
    "            print(\"Using DepthAnything for depth estimation.\")\n",
    "            depth_map = pipe(image)[\"depth\"] \n",
    "            depth_image = np.array(depth_map)\n",
    "      \n",
    "        \n",
    "        # Generate SAM proposals\n",
    "        proposals = generate_sam_proposals_project(\n",
    "            image,\n",
    "            sam_model,\n",
    "            sam_processor,\n",
    "            device,\n",
    "            grid_size=grid_size,\n",
    "            confidence_threshold=config.PROJECT_CONFIG['sam_confidence_threshold']\n",
    "        )\n",
    "\n",
    "        print(f\"generated {len(proposals)} proposals\")\n",
    "        \n",
    "        # Process each proposal\n",
    "        for proposal in proposals:\n",
    "            mask = proposal['mask']\n",
    "            \n",
    "            # Extract DINOv2 features\n",
    "            dino_features = extract_dino_features_from_segment(\n",
    "                image,\n",
    "                mask,\n",
    "                dino_model,\n",
    "                dino_processor,\n",
    "                device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops'] \n",
    "            )\n",
    "            \n",
    "            if dino_features is None:\n",
    "                continue\n",
    "\n",
    "            clip_features = extract_clip_features_from_segment(\n",
    "                image, mask, clip_model, clip_processor, device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops']\n",
    "            )\n",
    "            \n",
    "            # Get compact 3D position (Bounding 3D box + Centroid)\n",
    "            pos_data = extract_3d_position_from_mask(\n",
    "                mask,\n",
    "                depth_image,\n",
    "                frame_data['camera_intrinsics'],\n",
    "                frame_data['camera_pose'],\n",
    "                config.TSDF_CONFIG['depth_scale']\n",
    "            )\n",
    "            if pos_data is None:\n",
    "                continue\n",
    "            \n",
    "            results['objects'].append({\n",
    "                'frame_metadata': frame_data,\n",
    "                'features': dino_features,\n",
    "                'clip_features': clip_features,\n",
    "                'centroid': pos_data['centroid'],\n",
    "                'bbox_3d': pos_data['bbox_3d'],\n",
    "                'area_px': proposal['area'],\n",
    "                'confidence': proposal['confidence']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_data['frame_name']}: {e}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312b447",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a11efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection\n",
    "def project_world_to_image(K: np.ndarray, pts_world: np.ndarray, camera_pose: np.ndarray, img_shape: tuple):\n",
    "    \"\"\"\n",
    "    Project 3D points in world coordinates to 2D image pixels.\n",
    "\n",
    "    Args:\n",
    "        K: Camera intrinsics (3x3)\n",
    "        pts_world: Nx3 array of points in world coordinates\n",
    "        camera_pose: 4x4 camera pose (world → camera)\n",
    "        img_shape: (height, width) of the image\n",
    "\n",
    "    Returns:\n",
    "        List of (u, v) coordinates and a mask indicating if the point is inside the image\n",
    "    \"\"\"\n",
    "    pts_h = np.hstack([pts_world, np.ones((pts_world.shape[0], 1))])  # Nx4\n",
    "    # Transform world → camera\n",
    "    pts_cam = (camera_pose @ pts_h.T).T[:, :3]\n",
    "\n",
    "    uv_coords = []\n",
    "    inside_mask = []\n",
    "\n",
    "    H, W = img_shape\n",
    "\n",
    "    for X, Y, Z in pts_cam:\n",
    "        # Project using intrinsics\n",
    "        u = (K[0, 0] * X / Z) + K[0, 2]\n",
    "        v = (K[1, 1] * Y / Z) + K[1, 2]\n",
    "\n",
    "        uv_coords.append((u, v))\n",
    "        # Check if inside image boundaries\n",
    "        inside_mask.append(0 <= u < W and 0 <= v < H)\n",
    "\n",
    "    return uv_coords, np.array(inside_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera poses\n",
    "camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "\n",
    "def get_frame_list(config, frame_skip=1, max_frames=25, start_frame_name=None):\n",
    "    files = sorted(os.listdir(config.RGB_PATH))\n",
    "    frames = []\n",
    "\n",
    "    # If start_frame_name is provided, find its index\n",
    "    if start_frame_name is not None:\n",
    "        try:\n",
    "            start_idx = next(i for i, f in enumerate(files) if f == start_frame_name)\n",
    "            files = files[start_idx:]  # start from that frame\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"Start frame {start_frame_name} not found in {config.RGB_PATH}\")\n",
    "\n",
    "    for i, f in enumerate(files[::frame_skip]):\n",
    "        if i >= max_frames:\n",
    "            break\n",
    "\n",
    "        timestamp = os.path.splitext(f)[0]\n",
    "\n",
    "        # Load intrinsics for this frame\n",
    "        try:\n",
    "            K, image_size = load_camera_intrinsics(config.INTRINSICS_PATH, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}, failed to load intrinsics: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load camera pose\n",
    "        pose = camera_poses.get(timestamp, np.eye(4))\n",
    "\n",
    "        frames.append({\n",
    "            \"frame_name\": f,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"rgb_path\": os.path.join(config.RGB_PATH, f),\n",
    "            \"depth_path\": os.path.join(config.DEPTH_PATH, f.replace(\".jpg\", \".png\")),\n",
    "            \"camera_intrinsics\": K,\n",
    "            \"intrinsics_size\": image_size,\n",
    "            \"camera_pose\": pose\n",
    "        })\n",
    "\n",
    "    return frames\n",
    "\n",
    "def project_points(K, points_3d):\n",
    "    \"\"\"Project 3D points to 2D for visualization.\"\"\"\n",
    "    uv_coords = []\n",
    "    for X, Y, Z in points_3d:\n",
    "        if Z > 0:\n",
    "            uv = K @ np.array([X, Y, Z])\n",
    "            u, v = uv[0] / uv[2], uv[1] / uv[2]\n",
    "            uv_coords.append((u, v))\n",
    "    return uv_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811ddb1",
   "metadata": {},
   "source": [
    "## High-Level Interface Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83e2ce",
   "metadata": {},
   "source": [
    "### Frame Analysis\n",
    "\n",
    "The `analyse_frames()` function processes a sequence of frames to extract objects and their features for tracking.\n",
    "\n",
    "#### **Configuration:**\n",
    "- **n_frames**: Number of frames to process (default: 30)\n",
    "- **start_frame**: Starting frame filename (e.g., \"47333473_58534.757.png\")\n",
    "- **frame_skip**: Uses config setting to skip frames for efficiency\n",
    "\n",
    "#### **Processing Pipeline:**\n",
    "1. **Load models once** - SAM, DINOv2, and CLIP models loaded efficiently at startup\n",
    "2. **Get frame metadata** - Camera poses, intrinsics, and file paths for each frame\n",
    "3. **Process each frame** - Run the complete pipeline:\n",
    "   - Generate SAM object proposals\n",
    "   - Extract DINOv2 features for tracking\n",
    "   - Extract CLIP features for text queries\n",
    "   - Calculate 3D positions from depth data\n",
    "4. **Package results** - Structured data ready for the ObjectTracker\n",
    "\n",
    "#### **Output:**\n",
    "- **frame_results**: List of frames, each containing detected objects with features and 3D positions\n",
    "- **device**: GPU device handle for subsequent tracking operations\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Model reuse**: Load expensive models once, not per frame\n",
    "- **Structured output**: Consistent format for all tracking functions\n",
    "- **Flexible sequences**: Start from any frame, process any number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_frames(config, n_frames=30, frame_skip = 1, start_frame=\"47333473_58549.751.png\"):\n",
    "    \"\"\"\n",
    "    Extract objects from multiple frames using SAM + DINO + CLIP.\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    sam_model, sam_processor, device = load_sam_model(model_size='base')\n",
    "    dino_model, dino_processor, _ = load_dino_model(device=device)\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "\n",
    "    # Get frame metadata\n",
    "    frames_metadata = get_frame_list(config, frame_skip=frame_skip, max_frames=n_frames, start_frame_name=start_frame)\n",
    "\n",
    "    all_results = []\n",
    "    for frame_data in tqdm(frames_metadata, desc=\"Processing frames\"):\n",
    "        results = process_frame_project(\n",
    "            frame_data,\n",
    "            sam_model, sam_processor,\n",
    "            dino_model, dino_processor,\n",
    "            clip_model, clip_processor,\n",
    "            device,\n",
    "            config\n",
    "        )\n",
    "        results[\"frame_data\"] = frame_data\n",
    "        all_results.append(results)\n",
    "\n",
    "    return all_results, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d6557",
   "metadata": {},
   "source": [
    "### Tracking with Slider (Refactored)\n",
    "\n",
    "**Interactive Visualization**: The `query_with_slider()` function implements the complete tracking pipeline with an interactive slider for visual assessment. Your eyes are the ground truth here!\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "**1. Target Selection**\n",
    "- **With text query**: Uses CLIP similarity to find best matching object (e.g., \"ball\", \"sofa\")\n",
    "- **Without query**: Selects largest object (most pixels) for better tracking stability\n",
    "\n",
    "**2. Tracking Process**\n",
    "- **Initialize tracker** with target features and 3D bounding box from selected object\n",
    "- **For each frame**: Find best match using combined DINOv2 feature + 3D spatial similarity\n",
    "- **Update target** (if moving_target enabled) using exponential moving average for adaptation\n",
    "\n",
    "**3. Visualization**\n",
    "- **Bounding boxes** are projected from 3D world coordinates to 2D image space\n",
    "- **Interactive slider** lets you browse through frames to visually assess tracking quality\n",
    "- **Red rectangles** show the tracked object's projected 3D bounding box\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Visual assessment** is more reliable than automated metrics without ground truth\n",
    "- **Interactive exploration** lets you see exactly when tracking succeeds or fails\n",
    "- **3D-to-2D projection** provides spatial context from the world coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_slider(frame_results, device=None, query=None):\n",
    "    \"\"\"Track objects through frames with interactive slider - now using ObjectTracker class.\"\"\"\n",
    "    \n",
    "    if not frame_results:\n",
    "        print(\"Error: No frame results provided\")\n",
    "        return\n",
    "    \n",
    "    if not any(len(r['objects']) > 0 for r in frame_results):\n",
    "        print(\"Error: No objects found in any frame\")\n",
    "        return\n",
    "\n",
    "    if device is None:\n",
    "        _, _, device = load_sam_model(model_size='base')\n",
    "\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = ObjectTracker(config)\n",
    "\n",
    "    # Target selection logic\n",
    "    text_features_np = None\n",
    "    if query is not None:\n",
    "        text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features_np = text_features.cpu().numpy().squeeze()\n",
    "\n",
    "    # Pick object to track\n",
    "    if query is None or text_features_np is None:            \n",
    "        # Pick an object (first object with largest area for better tracking)\n",
    "        best_object_id, best_frame_id = 0, 0\n",
    "        best_area = 0\n",
    "        \n",
    "        # Look for substantial object in first few frames\n",
    "        for f_idx in range(min(3, len(frame_results))):\n",
    "            for o_idx, obj in enumerate(frame_results[f_idx]['objects']):\n",
    "                if obj['area_px'] > best_area:\n",
    "                    best_area = obj['area_px']\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "        \n",
    "    else:\n",
    "        # Pick object by CLIP similarity with query\n",
    "        best_score_overall = -1\n",
    "        best_frame_id, best_object_id = None, None\n",
    "        for f_idx, results in enumerate(frame_results):\n",
    "            for o_idx, obj in enumerate(results['objects']):\n",
    "                if obj['clip_features'] is None:\n",
    "                    continue\n",
    "                sim = float(np.dot(obj['clip_features'], text_features_np))\n",
    "                if sim > best_score_overall:\n",
    "                    best_score_overall = sim\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "\n",
    "        print(f\"Highest CLIP correlation with frame {best_frame_id} and object {best_object_id}.\")\n",
    "\n",
    "        if best_score_overall < 0:\n",
    "            print(\"No suitable object found for query.\")\n",
    "            return\n",
    "\n",
    "    # Set initial target\n",
    "    target_features = frame_results[best_frame_id]['objects'][best_object_id]['features']\n",
    "    target_bbox = frame_results[best_frame_id]['objects'][best_object_id]['bbox_3d']\n",
    "    target_centroid = frame_results[best_frame_id]['objects'][best_object_id]['centroid']\n",
    "    tracker.set_target(target_features, target_bbox, target_centroid)\n",
    "    \n",
    "    print(f\"Starting tracking from frame {best_frame_id}, object {best_object_id}\")\n",
    "    print(f\"Tracking config: moving_target={config.TRACKING_CONFIG['moving_target']}, threshold={config.TRACKING_CONFIG['similarity_threshold']}\")\n",
    "\n",
    "    # Process frames with tracker\n",
    "    processed_frames = []\n",
    "    print(f\"Processing frames ...\")\n",
    "    \n",
    "    for idx, results in enumerate(frame_results):\n",
    "        frame = Image.open(results['frame_data']['rgb_path']).convert(\"RGB\")\n",
    "        overlay = frame.convert('RGBA')\n",
    "        draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "\n",
    "        if results['objects']:\n",
    "            # Use tracker to find best match\n",
    "            match = tracker.find_best_match(results['objects'])\n",
    "            if match:\n",
    "                best_idx = match['index']\n",
    "                obj = results['objects'][best_idx]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                points_2d, inside_mask = project_world_to_image(\n",
    "                    results['frame_data']['camera_intrinsics'],\n",
    "                    obj['bbox_3d'],\n",
    "                    results['frame_data']['camera_pose'],\n",
    "                    img_shape=(frame.height, frame.width)\n",
    "                )\n",
    "\n",
    "                for (u, v), inside in zip(points_2d, inside_mask):\n",
    "                    print(f\"Projected point (u={u:.1f}, v={v:.1f}) → {'INSIDE' if inside else 'OUTSIDE'} [{frame.width}x{frame.height}]\")\n",
    "\n",
    "                if points_2d:\n",
    "                    xs, ys = zip(*points_2d)\n",
    "                    x0, x1 = int(min(xs)), int(max(xs))\n",
    "                    y0, y1 = int(min(ys)), int(max(ys))\n",
    "                    draw.rectangle([x0, y0, x1, y1], outline=(255,0,0,180), width=3)\n",
    "                \n",
    "                # Update target if moving target is enabled\n",
    "                tracker.update_target(obj['features'], obj['bbox_3d'], match['score'])\n",
    "\n",
    "        processed_frames.append(overlay)\n",
    "\n",
    "\n",
    "\n",
    "    # Create slider widget\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(processed_frames)-1,\n",
    "        step=1,\n",
    "        description='Frame'\n",
    "    )\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def show_frame(change):\n",
    "        idx = change['new']\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            display(processed_frames[idx])\n",
    "\n",
    "    slider.observe(show_frame, names='value')\n",
    "    display(widgets.VBox([slider, out]))\n",
    "    \n",
    "    # Show first frame by default\n",
    "    with out:\n",
    "        display(processed_frames[0])\n",
    "    \n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3d39a",
   "metadata": {},
   "source": [
    "## Configuration Toggle Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343add5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current configuration\n",
    "print(\"Current Tracking Configuration:\")\n",
    "for key, value in config.TRACKING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTo modify tracking behavior:\")\n",
    "print(\"  config.TRACKING_CONFIG['moving_target'] = True/False\")\n",
    "print(\"  config.TRACKING_CONFIG['similarity_threshold'] = 0.4-0.9\")\n",
    "print(\"  config.TRACKING_CONFIG['feature_weight'] = 0.5-0.9 (vs spatial weight)\")\n",
    "print(\"  config.TRACKING_CONFIG['update_momentum'] = 0.05-0.3 (if moving_target=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288f960",
   "metadata": {},
   "source": [
    "# 3D Visualization\n",
    "\n",
    "The **3D Visualization** module provides tools for displaying and analyzing tracked objects in a reconstructed 3D environment.  \n",
    "It integrates tracking data, ground truth annotations, and camera pose information to render a visual scene with bounding boxes and object labels.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "- **`track_objects(frame_results)`**  \n",
    "  Tracks multiple objects across sequential frames using feature matching and spatial information.  \n",
    "  It initializes object trackers for the first frame and updates or adds new trackers as new frames are processed.\n",
    "\n",
    "- **`trackers_to_viz_format(trackers, aligned_frames, labels=None)`**  \n",
    "  Converts tracked object data into a visualization-friendly format, including:\n",
    "  - 3D bounding box coordinates  \n",
    "  - Object centroids  \n",
    "  - Labels, confidence scores, and colors  \n",
    "\n",
    "- **`object_by_query(query, trackers, average_clip=False)`**  \n",
    "  Selects the object that best matches a given **text query** using CLIP-based feature similarity.  \n",
    "  Optionally averages features across frames for improved robustness.\n",
    "\n",
    "- **`query_3d_visualization(labels, trackers, max_frames=25, average_clip=False, start_frame_name=\"...\")`**  \n",
    "  The main visualization routine that:\n",
    "  1. Loads ground truth data using `load_ground_truth_data()`.  \n",
    "  2. Loads and aligns RGB-D frames with `get_frame_list()` and `validate_and_align_frame_data()`.  \n",
    "  3. Identifies tracked objects based on user queries through `object_by_query()`.  \n",
    "  4. Converts object data into the proper format using `trackers_to_viz_format()`.  \n",
    "  5. Visualizes results in 3D with `visualize_3d_scene_bbox_results()`.\n",
    "\n",
    "- **`visualize_3d_scene_bbox_results()`**  \n",
    "  Renders the 3D scene, displaying both ground truth annotations and detected object bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils.viz_eval import visualize_3d_scene_bbox_results\n",
    "from lab_utils.ground_truth import load_ground_truth_data\n",
    "\n",
    "gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "    config.SCENE_ID,\n",
    "    config.BASE_PATH,\n",
    "    config.GT_CONFIG\n",
    ")\n",
    "\n",
    "def track_objects(frame_results):\n",
    "    \"\"\"Track multiple objects across frames.\"\"\"\n",
    "    trackers = []\n",
    "    for frame_id, frame in enumerate(frame_results):\n",
    "        objs = frame['objects']\n",
    "\n",
    "        if not objs:\n",
    "            continue\n",
    "\n",
    "        if not trackers:\n",
    "            # First frame: create one tracker per object\n",
    "            for obj in objs:\n",
    "                tracker = ObjectTracker(config)\n",
    "                tracker.set_target(obj['features'], obj['bbox_3d'], obj['centroid'], clip_features = obj['clip_features'])\n",
    "                trackers.append(tracker)\n",
    "        else:\n",
    "            for obj in objs:\n",
    "                tracker = ObjectTracker(config)\n",
    "                tracker.set_target(obj['features'], obj['bbox_3d'], obj['centroid'], clip_features = obj['clip_features'])\n",
    "                \n",
    "                match = tracker.find_best_match(trackers)\n",
    "                \n",
    "                if match:\n",
    "                    match_id = match['index']\n",
    "                    trackers[match_id].update_target(obj['features'], obj['bbox_3d'], match['score'], new_clip_features = obj['clip_features'])\n",
    "                    \n",
    "                else:\n",
    "                    # No match -> create new tracker\n",
    "                    trackers.append(tracker)\n",
    "    print(\"Final length trackers\", len(trackers))\n",
    "    return trackers\n",
    "\n",
    "\n",
    "def trackers_to_viz_format(trackers, aligned_frames, labels=None):\n",
    "    detections_3d = []\n",
    "    transform = np.eye(4)\n",
    "\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        if tracker.target_bbox is None or tracker.target_centroid is None:\n",
    "            continue\n",
    "\n",
    "        # Convert bbox corners to homogeneous coordinates\n",
    "        bbox_min_h = np.append(tracker.target_bbox[0], 1.0)\n",
    "        bbox_max_h = np.append(tracker.target_bbox[1], 1.0)\n",
    "        centroid_h = np.append(tracker.target_centroid, 1.0)\n",
    "\n",
    "        # Apply transform\n",
    "        bbox_min_trans = (transform @ bbox_min_h)[:3]\n",
    "        bbox_max_trans = (transform @ bbox_max_h)[:3]\n",
    "        centroid_trans = (transform @ centroid_h)[:3]\n",
    "\n",
    "        if labels is not None:\n",
    "            label = labels[i]\n",
    "        else: \n",
    "            label = f\"Object_{i}\"\n",
    "\n",
    "        obj_dict = {\n",
    "            'bbox_3d_world': np.stack([bbox_min_trans, bbox_max_trans]),\n",
    "            'centroid': centroid_trans,\n",
    "            'label': label,\n",
    "            'score': 1.0,\n",
    "            'color': (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "        }\n",
    "\n",
    "        detections_3d.append(obj_dict)\n",
    "\n",
    "    return detections_3d\n",
    "\n",
    "def object_by_query(query, trackers, average_clip = False):\n",
    "\n",
    "    _, _, device = load_sam_model(model_size='base')\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = ObjectTracker(config)\n",
    "\n",
    "    # Target selection logic\n",
    "    text_features_np = None\n",
    "    if query is not None:\n",
    "        text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features_np = text_features.cpu().numpy().squeeze()\n",
    "\n",
    "    # Pick object by CLIP similarity with query\n",
    "    best_score_overall = -1\n",
    "\n",
    "    for o_idx, obj in enumerate(trackers):\n",
    "        if obj.clip_features is None:\n",
    "            continue\n",
    "\n",
    "        if average_clip:\n",
    "            clip_feats = np.array(obj.clip_features)\n",
    "            if clip_feats.ndim == 2:\n",
    "                # Average across all stored features (e.g., across frames)\n",
    "                mean_feat = clip_feats.mean(axis=0)\n",
    "            else:\n",
    "                mean_feat = clip_feats.flatten()\n",
    "\n",
    "            # Normalize before similarity\n",
    "            mean_feat /= np.linalg.norm(mean_feat) + 1e-8\n",
    "\n",
    "            sim = float(np.dot(mean_feat, text_features_np))\n",
    "            if sim > best_score_overall:\n",
    "                best_score_overall = sim\n",
    "                best_object_id = o_idx\n",
    "        else:\n",
    "            for clip_feature in obj.clip_features:\n",
    "                clip_feature = clip_feature / np.linalg.norm(clip_feature)\n",
    "                sim = float(np.dot(clip_feature, text_features_np))\n",
    "                if sim > best_score_overall:\n",
    "                    best_score_overall = sim\n",
    "                    best_object_id = o_idx\n",
    "\n",
    "    if best_score_overall < 0:\n",
    "        print(\"No suitable object found for query.\")\n",
    "        return\n",
    "    \n",
    "    return best_object_id\n",
    "\n",
    "\"\"\"\n",
    "Data loading and processing utilities for 3D Semantic Object Mapping Lab.\n",
    "\n",
    "This module contains functions for loading and validating RGB-D frame data,\n",
    "camera intrinsics, camera poses, and aligning temporal data from ARKitScenes dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from scipy.spatial.transform import Rotation\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def get_frame_list(config, frame_skip=1, max_frames=25, start_frame_name=None):\n",
    "    print(f\"getting frame list with config {config},\\n frame_skip {frame_skip},\\n max_frames {max_frames},\\n start_frame_name {start_frame_name}\")\n",
    "    files = sorted(os.listdir(config.RGB_PATH))\n",
    "    frames = []\n",
    "\n",
    "    # If start_frame_name is provided, find its index\n",
    "    if start_frame_name is not None:\n",
    "        try:\n",
    "            start_idx = next(i for i, f in enumerate(files) if f == start_frame_name)\n",
    "            files = files[start_idx:]  # start from that frame\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"Start frame {start_frame_name} not found in {config.RGB_PATH}\")\n",
    "\n",
    "    for i, f in enumerate(files[::frame_skip]):\n",
    "        if i >= max_frames:\n",
    "            break\n",
    "\n",
    "        timestamp = os.path.splitext(f)[0]\n",
    "\n",
    "        # Load intrinsics for this frame\n",
    "        try:\n",
    "            K, image_size = load_camera_intrinsics(config.INTRINSICS_PATH, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}, failed to load intrinsics: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load camera pose\n",
    "        camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "        camera_pose = camera_poses.get(timestamp)\n",
    "        if camera_pose is None:\n",
    "            # Try to find the closest timestamp\n",
    "            timestamp_float = float(str(timestamp).split(\"_\")[-1])\n",
    "            pose_timestamps = [(t, abs(timestamp_float - float(t))) for t in camera_poses.keys()]\n",
    "            pose_timestamps.sort(key=lambda x: x[1])\n",
    "            if pose_timestamps and pose_timestamps[0][1] < 0.5:  # or whatever your tolerance is\n",
    "                closest_timestamp = pose_timestamps[0][0]\n",
    "                camera_pose = camera_poses[closest_timestamp]\n",
    "            else:\n",
    "                print(f\"No close camera pose found for {timestamp}, using identity.\")\n",
    "                camera_pose = np.eye(4)\n",
    "\n",
    "        frames.append({\n",
    "            \"frame_name\": f,\n",
    "            \"filename\": f,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"rgb_path\": os.path.join(config.RGB_PATH, f),\n",
    "            \"depth_path\": os.path.join(config.DEPTH_PATH, f.replace(\".jpg\", \".png\")),\n",
    "            \"camera_intrinsics\": K,\n",
    "            \"intrinsics_size\": image_size,\n",
    "            \"camera_pose\": camera_pose\n",
    "        })\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_3d_visualization(labels, trackers, max_frames = 25, average_clip = False, start_frame_name =\"47333473_58528.259.png\"):\n",
    "    gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "        config.SCENE_ID, \n",
    "        config.BASE_PATH,\n",
    "        config.GT_CONFIG\n",
    "    )\n",
    "\n",
    "    camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "\n",
    "    frames_metadata = get_frame_list(config, frame_skip=config.TSDF_CONFIG['frame_skip'], max_frames=max_frames, start_frame_name=start_frame_name)\n",
    "\n",
    "    for frame_metadata in frames_metadata:\n",
    "        frame_name = frame_metadata['frame_name'] # e.g., \"47333473_58528.259.png\"\n",
    "        frame_metadata['filename'] = frame_name  \n",
    "        timestamp_str = frame_name.split('_')[1].replace('.png', '')  # \"58528.259\"\n",
    "        frame_metadata['timestamp'] = float(timestamp_str)\n",
    "\n",
    "    aligned_frames = validate_and_align_frame_data(\n",
    "            frames_metadata, camera_poses, config.RGB_PATH, \n",
    "            config.DEPTH_PATH, config.INTRINSICS_PATH, timestamp_tolerance=0.1\n",
    "        )\n",
    "\n",
    "    objects = []\n",
    "    for label in labels:\n",
    "        objects.append(trackers[object_by_query(label, trackers, average_clip=average_clip)])\n",
    "\n",
    "    detections_3d = trackers_to_viz_format(\n",
    "        objects, \n",
    "        aligned_frames, \n",
    "        labels,\n",
    "        )\n",
    "\n",
    "    tsdf_point_cloud = build_tsdf_point_cloud(config, max_frames_for_mapping=596, use_cached=True)\n",
    "\n",
    "    visualize_3d_scene_bbox_results(\n",
    "        point_cloud=None,  # optionally pass TSDF / frame point cloud\n",
    "        detections_3d=detections_3d,\n",
    "        gt_annotations=gt_annotations,\n",
    "        title=f\"Tracked Objects in World Frame\"\n",
    "    )\n",
    "\n",
    "    return detections_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2f41e",
   "metadata": {},
   "source": [
    "\n",
    "## Running the Visualization\n",
    "\n",
    "The visualization process combines tracking, querying, and rendering steps to create a full 3D scene with labeled objects.\n",
    "\n",
    "Run the cell below to run the visualization.\n",
    "\n",
    "Use either:\n",
    "\n",
    "**`frame_results_many, device_id = analyse_frames(config, n_frames=200, frame_skip = 3, start_frame=\"47333473_58548.751.png\")`**\n",
    "\n",
    "or\n",
    "\n",
    "**`_frame_results_many = load_results()_`**\n",
    "\n",
    "to generate frame data or load pre-computed results.\n",
    "\n",
    "Edit the list of labels to add/remove queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31bdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.TRACKING_CONFIG['moving_target'] = True\n",
    "config.TRACKING_CONFIG[\"use_depth\"] = True\n",
    "\n",
    "camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "\n",
    "# Either uncomment line below or use the functions below to load example data:\n",
    "#frame_results_many, device_id = analyse_frames(config, n_frames=200, frame_skip = 3, start_frame=\"47333473_58548.751.png\")\n",
    "frame_results_many = load_results()\n",
    "\n",
    "trackers = track_objects(frame_results_many)\n",
    "labels = [\"bed\", \"window\", \"ball\", \"curtain\", \"stool\", \"pillow\", \"cupboard\", \"shelf\", \"wall-mounted cabinet\", \"two-person sofa\"]\n",
    "\n",
    "detections_3d = query_3d_visualization(labels, trackers, max_frames = 110, average_clip=False)#, start_frame_name = \"47333473_58548.751.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e46adb",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detections_all_matches(gt_annotations, detections_3d, det_to_gt_map):\n",
    "    \"\"\"\n",
    "    Compare GT objects to detected objects.\n",
    "    For each GT object, add all detected objects with the same label.\n",
    "    Computes centroid distance and IoU (3D bbox overlap) for each match.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for det in detections_3d:\n",
    "        det_centroid = np.array(det['centroid'], dtype=np.float32)\n",
    "        det_bbox_min = det['bbox_3d_world'][0]\n",
    "        det_bbox_max = det['bbox_3d_world'][1]\n",
    "\n",
    "        matched_gts = [gt for gt in gt_annotations if gt['label'] == det_to_gt_map[det['label']]]\n",
    "\n",
    "        best_index = -1\n",
    "        best_distance = np.inf\n",
    "        for i, gt_obj in enumerate(matched_gts):\n",
    "            \n",
    "\n",
    "            gt_label = gt_obj['label']\n",
    "            gt_centroid = np.array(gt_obj['centroid'], dtype=np.float32)\n",
    "            \n",
    "\n",
    "            # Compute centroid distance\n",
    "            centroid_dist = np.linalg.norm(gt_centroid - det_centroid)\n",
    "\n",
    "            if(centroid_dist < best_distance):\n",
    "                best_index = i\n",
    "                best_distance = centroid_dist\n",
    "\n",
    "        gt_obj = matched_gts[best_index]\n",
    "        gt_bbox_min = np.min(gt_obj['corners'], axis=0)\n",
    "        gt_bbox_max = np.max(gt_obj['corners'], axis=0)\n",
    "\n",
    "        # Compute 3D IoU\n",
    "        overlap_min = np.maximum(gt_bbox_min, det_bbox_min)\n",
    "        overlap_max = np.minimum(gt_bbox_max, det_bbox_max)\n",
    "        overlap = np.clip(overlap_max - overlap_min, a_min=0, a_max=None)\n",
    "        intersection = np.prod(overlap)\n",
    "        vol_gt = np.prod(gt_bbox_max - gt_bbox_min)\n",
    "        vol_det = np.prod(det_bbox_max - det_bbox_min)\n",
    "        union = vol_gt + vol_det - intersection\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "\n",
    "        results.append({\n",
    "            'gt_label': gt_label,\n",
    "            'det_label': det['label'],\n",
    "            'centroid_dist': float(best_distance),\n",
    "            'iou': float(iou)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Loads example results\n",
    "def load_results():\n",
    "    pickle_file = \"frame_results.pkl\"\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        frame_results_many = pickle.load(f)\n",
    "        return frame_results_many\n",
    "\n",
    "# Save results to file\n",
    "def save_result(frame_results):\n",
    "    pickle_file = \"frame_results.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(frame_results, f)\n",
    "    print(f\"Saved frame_results to {pickle_file} (Pickle)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63020001",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"bed\", \"shelf\", \"closet with doors\", \"stool\", \"two-person sofa\", \"hanging shelf\"]\n",
    "detections_3d = query_3d_visualization(labels, trackers, max_frames = 110, average_clip=False)\n",
    "\n",
    "det_to_gt_map = {\n",
    "    \"bed\": \"bed\",\n",
    "    \"two-person sofa\": \"sofa\",\n",
    "    \"brown velvet\": \"stool\",\n",
    "    \"hanging shelf\": \"cabinet\",\n",
    "    \"closet with doors\": \"cabinet\",\n",
    "    \"stool\": \"stool\",\n",
    "    \"shelf\": \"cabinet\"\n",
    "}\n",
    "\n",
    "gt_annotations, gt_mesh = load_ground_truth_data(\n",
    "        config.SCENE_ID, \n",
    "        config.BASE_PATH,\n",
    "        config.GT_CONFIG\n",
    "    )\n",
    "\n",
    "evaluate_detections_all_matches(gt_annotations, detections_3d, det_to_gt_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
