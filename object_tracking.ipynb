{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671dd972",
   "metadata": {},
   "source": [
    "# Object Tracking Notebook\n",
    "This notebook combines approaches from Lab2_A, Lab2_C, and Lab2_E to perform object tracking instead of voxel tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb4df9",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import all necessary libraries and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454287f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segment-anything\n",
      "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting clip-by-openai\n",
      "Collecting clip-by-openai\n",
      "  Downloading clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
      "  Downloading clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
      "Collecting pillow\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-11.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (62 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (62 kB)\n",
      "Collecting ftfy (from clip-by-openai)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting ftfy (from clip-by-openai)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting regex (from clip-by-openai)\n",
      "  Downloading regex-2025.9.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting regex (from clip-by-openai)\n",
      "  Downloading regex-2025.9.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from clip-by-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting clip-by-openai\n",
      "Collecting tqdm (from clip-by-openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting clip-by-openai\n",
      "  Downloading clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
      "  Downloading clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
      "  Downloading clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torch\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp313-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp313-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1, clip-by-openai==1.1, torch and torchvision==0.21.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested torch\n",
      "    torchvision 0.21.0 depends on torch==2.6.0\n",
      "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1, clip-by-openai==1.1, torch and torchvision==0.21.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested torch\n",
      "    torchvision 0.21.0 depends on torch==2.6.0\n",
      "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpip install segment-anything torch torchvision clip-by-openai pillow matplotlib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Dependencies and Imports\n",
    "# Install required packages if not already installed\n",
    "!pip install segment-anything torch torchvision clip-by-openai pillow matplotlib\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# SAM and CLIP imports\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import clip\n",
    "\n",
    "# Utility imports (if available)\n",
    "from lab_utils.data_utils import *\n",
    "from lab_utils.detection_utils import *\n",
    "from lab_utils.visualization_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e7874",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "Load scene data, images, and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (update paths and logic as needed)\n",
    "import os\n",
    "scene_id = '40753679'\n",
    "data_path = f'ARKitScenesData/{scene_id}/40753679_frames/lowres_wide/'\n",
    "\n",
    "# Get all image file paths (assuming .png images)\n",
    "images = [os.path.join(data_path, fname) for fname in os.listdir(data_path) if fname.endswith('.png')]\n",
    "images.sort()  # Ensure temporal order\n",
    "\n",
    "# If you have annotation loading logic, add it here\n",
    "# annotations = load_annotations(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Check loaded images\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "print(\"First 3 image paths:\", images[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad89bd",
   "metadata": {},
   "source": [
    "## 3. Object Detection\n",
    "Detect objects in the scene using methods from previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9003b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Load SAM and CLIP models\n",
    "def load_sam_clip(sam_checkpoint_path):\n",
    "    from segment_anything import SamPredictor, sam_model_registry\n",
    "    import clip\n",
    "    import torch\n",
    "    sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint_path)\n",
    "    predictor = SamPredictor(sam)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    return predictor, clip_model, preprocess, device\n",
    "\n",
    "sam_checkpoint_path = \"path/to/sam_checkpoint.pth\"\n",
    "predictor, clip_model, preprocess, device = load_sam_clip(sam_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6647ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Check SAM and CLIP model loading\n",
    "print(\"Predictor type:\", type(predictor))\n",
    "print(\"CLIP model type:\", type(clip_model))\n",
    "print(\"Preprocess type:\", type(preprocess))\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39630cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Detect objects in images using SAM + CLIP\n",
    "def detect_objects_sam_clip(images, predictor, clip_model, preprocess, device, text_prompts):\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    detected_objects = []\n",
    "    text_tokens = clip.tokenize(text_prompts).to(device)\n",
    "    for img_path in images:\n",
    "        image = np.array(Image.open(img_path))\n",
    "        predictor.set_image(image)\n",
    "        masks, _, _ = predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            multimask_output=True\n",
    "        )\n",
    "        for mask in masks:\n",
    "            y_indices, x_indices = np.where(mask)\n",
    "            if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "                continue\n",
    "            y_min, y_max = y_indices.min(), y_indices.max()\n",
    "            x_min, x_max = x_indices.min(), x_indices.max()\n",
    "            crop = image[y_min:y_max, x_min:x_max]\n",
    "            pil_crop = Image.fromarray(crop)\n",
    "            clip_input = preprocess(pil_crop).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(clip_input)\n",
    "                text_features = clip_model.encode_text(text_tokens)\n",
    "                logits_per_image = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "                pred_class = text_prompts[logits_per_image.argmax().item()]\n",
    "            detected_objects.append({\n",
    "                \"image\": img_path,\n",
    "                \"bbox\": [x_min, y_min, x_max, y_max],\n",
    "                \"mask\": mask,\n",
    "                \"class\": pred_class\n",
    "            })\n",
    "    return detected_objects\n",
    "\n",
    "# Example usage:\n",
    "text_prompts = [\"sofa\", \"shelf\", \"table\", \"chair\"]\n",
    "detected_objects = detect_objects_sam_clip(images, predictor, clip_model, preprocess, device, text_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe330d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Check detection output\n",
    "print(f\"Detected {len(detected_objects)} objects.\")\n",
    "if detected_objects:\n",
    "    print(\"First detected object:\", detected_objects[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebaeb9e",
   "metadata": {},
   "source": [
    "## 4. Object Tracking Logic\n",
    "Implement object tracking across frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Define object tracking function (IoU-based, no voxels)\n",
    "def track_objects(detected_objects, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Track objects across frames using IoU and class consistency.\n",
    "    \"\"\"\n",
    "    # Sort detected objects by image path to ensure temporal order\n",
    "    detected_objects.sort(key=lambda x: x[\"image\"])\n",
    "    # Group detections by frame\n",
    "    frames = {}\n",
    "    for obj in detected_objects:\n",
    "        img_path = obj[\"image\"]\n",
    "        if img_path not in frames:\n",
    "            frames[img_path] = []\n",
    "        frames[img_path].append(obj)\n",
    "    frame_list = sorted(frames.keys())\n",
    "    detections_by_frame = [frames[f] for f in frame_list]\n",
    "    next_track_id = 0\n",
    "    tracked_objects = []\n",
    "    # First frame: assign new track IDs\n",
    "    if detections_by_frame:\n",
    "        for detection in detections_by_frame[0]:\n",
    "            detection[\"track_id\"] = next_track_id\n",
    "            tracked_objects.append(detection)\n",
    "            next_track_id += 1\n",
    "    # Subsequent frames: associate detections\n",
    "    for frame_idx in range(1, len(detections_by_frame)):\n",
    "        current_detections = detections_by_frame[frame_idx]\n",
    "        prev_frame_tracks = [obj for obj in tracked_objects if obj[\"image\"] == frame_list[frame_idx-1]]\n",
    "        for detection in current_detections:\n",
    "            best_iou = iou_threshold\n",
    "            best_match = -1\n",
    "            for track in prev_frame_tracks:\n",
    "                if track[\"class\"] != detection[\"class\"]:\n",
    "                    continue\n",
    "                # IoU calculation\n",
    "                det_bbox = detection[\"bbox\"]\n",
    "                track_bbox = track[\"bbox\"]\n",
    "                x1 = max(det_bbox[0], track_bbox[0])\n",
    "                y1 = max(det_bbox[1], track_bbox[1])\n",
    "                x2 = min(det_bbox[2], track_bbox[2])\n",
    "                y2 = min(det_bbox[3], track_bbox[3])\n",
    "                if x2 < x1 or y2 < y1:\n",
    "                    iou = 0\n",
    "                else:\n",
    "                    intersection = (x2 - x1) * (y2 - y1)\n",
    "                    det_area = (det_bbox[2] - det_bbox[0]) * (det_bbox[3] - det_bbox[1])\n",
    "                    track_area = (track_bbox[2] - track_bbox[0]) * (track_bbox[3] - track_bbox[1])\n",
    "                    union = det_area + track_area - intersection\n",
    "                    iou = intersection / union if union > 0 else 0\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_match = track[\"track_id\"]\n",
    "            if best_match >= 0:\n",
    "                detection[\"track_id\"] = best_match\n",
    "            else:\n",
    "                detection[\"track_id\"] = next_track_id\n",
    "                next_track_id += 1\n",
    "            tracked_objects.append(detection)\n",
    "    return tracked_objects\n",
    "\n",
    "# 4.2 Apply tracking to detected objects\n",
    "tracked_objects = track_objects(detected_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04069b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Check tracking output\n",
    "print(f\"Tracked {len(tracked_objects)} objects.\")\n",
    "if tracked_objects:\n",
    "    print(\"First tracked object:\", tracked_objects[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4fcb7",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Visualize tracked objects over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tracking results\n",
    "# visualize_tracking(tracked_objects, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969c149",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Evaluate tracking performance using ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54266a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tracking\n",
    "# evaluation_metrics = evaluate_tracking(tracked_objects, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
