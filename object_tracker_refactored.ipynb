{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60d9d7d8",
   "metadata": {},
   "source": [
    "# Project part E - Refactored with ObjectTracker Class\n",
    "\n",
    "Static object tracking through multiple frames using SAM and DINOv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930be79c",
   "metadata": {},
   "source": [
    "## Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers==4.44.0 huggingface-hub==0.24.0 pillow numpy opencv-python open3d ipympl rerun-sdk[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Import lab utility functions\n",
    "from lab_utils.data_utils import get_frame_list, load_camera_poses, validate_and_align_frame_data, load_camera_intrinsics\n",
    "from lab_utils.tsdf_utils import build_tsdf_point_cloud\n",
    "from lab_utils.model_loaders import load_sam_model, load_clip_model, load_dino_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae4e82",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Scene Configuration\n",
    "    SCENE_ID = \"47333473\"\n",
    "    BASE_PATH = f\"ARKitScenesData/{SCENE_ID}/{SCENE_ID}_frames\"\n",
    "    RGB_PATH = os.path.join(BASE_PATH, \"lowres_wide\")\n",
    "    DEPTH_PATH = os.path.join(BASE_PATH, \"lowres_depth\")\n",
    "    INTRINSICS_PATH = os.path.join(BASE_PATH, \"lowres_wide_intrinsics\")\n",
    "    TRAJ_FILE_PATH = os.path.join(BASE_PATH, \"lowres_wide.traj\")\n",
    "    \n",
    "    # Project Configuration\n",
    "    PROJECT_CONFIG = {\n",
    "        'frame_skip': 7,                                  \n",
    "        'max_frames': 40,                                  \n",
    "        'grid_size': 6,                                    \n",
    "        'sam_confidence_threshold': 0.5,                   \n",
    "        'clip_model': 'openai/clip-vit-base-patch32',      \n",
    "        'example_viz_index': 29,                          \n",
    "        'padding_ratio_image_crops': 0.1                   \n",
    "    }\n",
    "    \n",
    "    # Tracking Configuration (NEW)\n",
    "    TRACKING_CONFIG = {\n",
    "        'similarity_threshold': 0.6,      # Minimum score to consider a match\n",
    "        'feature_weight': 0.8,            # Weight for feature similarity (alpha)\n",
    "        'spatial_weight': 0.2,            # Weight for spatial similarity (1-alpha)\n",
    "        'moving_target': False,           # Enable target updates\n",
    "        'update_momentum': 0.1,           # How much to update target (0.1 = conservative)\n",
    "        'confidence_threshold': 0.8       # Minimum confidence for target update\n",
    "    }\n",
    "    \n",
    "    # TSDF Configuration (for visualization)\n",
    "    TSDF_CONFIG = {\n",
    "        'frame_skip': 3,\n",
    "        'depth_scale': 1000.0,\n",
    "        'depth_trunc': 7.0,\n",
    "        'voxel_size': 0.04,\n",
    "        'batch_size': 20,\n",
    "        'max_frames': 1000,\n",
    "        'volume_length': 30.0,\n",
    "        'resolution': 512,\n",
    "    }\n",
    "    \n",
    "    # Rerun Viewer Dimensions\n",
    "    RERUN_WIDTH, RERUN_HEIGHT = 1200, 700\n",
    "\n",
    "# Create and validate config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43129b8f",
   "metadata": {},
   "source": [
    "## Core Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efb543",
   "metadata": {},
   "source": [
    "### Object Tracker Class\n",
    "\n",
    "The `ObjectTracker` class encapsulates all the tracking logic in a clean interface. It handles:\n",
    "\n",
    "- **Target Management**: Setting and updating the reference object features\n",
    "- **Configuration**: Using the centralized tracking config for all parameters\n",
    "- **Similarity Matching**: Combining DINOv2 features with 3D spatial information  \n",
    "- **Moving Target Updates**: Optionally adapting to appearance changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64596c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    \"\"\"Lightweight object tracker for multi-frame tracking.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.target_features = None\n",
    "        self.target_bbox = None\n",
    "        self.tracking_history = []  # Optional: for debugging/analysis\n",
    "        \n",
    "    def set_target(self, features, bbox_3d):\n",
    "        \"\"\"Set the initial target to track.\"\"\"\n",
    "        self.target_features = features.copy() if hasattr(features, 'copy') else np.array(features)\n",
    "        self.target_bbox = bbox_3d.copy() if hasattr(bbox_3d, 'copy') else np.array(bbox_3d)\n",
    "        \n",
    "    def find_best_match(self, objects):\n",
    "        \"\"\"Find the best matching object in the current frame.\"\"\"\n",
    "        if self.target_features is None:\n",
    "            return None\n",
    "            \n",
    "        if not objects:\n",
    "            return None\n",
    "            \n",
    "        best_idx = None\n",
    "        best_score = 0.0\n",
    "        similarity_threshold = self.config.TRACKING_CONFIG['similarity_threshold']\n",
    "        alpha = self.config.TRACKING_CONFIG['feature_weight']\n",
    "\n",
    "        for i, obj in enumerate(objects):\n",
    "            features = obj['features']\n",
    "            bbox_3d = obj['bbox_3d']\n",
    "            centroid = obj['centroid']\n",
    "            \n",
    "            # Hard distance cutoff \n",
    "            target_centroid = (self.target_bbox[0] + self.target_bbox[1]) / 2.0\n",
    "            distance = np.linalg.norm(centroid - target_centroid)\n",
    "            if distance > 0.5:  # Hard threshold\n",
    "                continue\n",
    "            \n",
    "            # Feature similarity (cosine similarity)\n",
    "            sim_features = np.dot(self.target_features, features) / (\n",
    "                np.linalg.norm(self.target_features) * np.linalg.norm(features) + 1e-8\n",
    "            )\n",
    "            \n",
    "            # Spatial similarity (3D IoU)\n",
    "            sim_spatial = self._bbox3d_iou(self.target_bbox, bbox_3d)\n",
    "\n",
    "            # Combined score\n",
    "            score = alpha * sim_features + (1 - alpha) * sim_spatial\n",
    "            \n",
    "            if score > best_score and score >= similarity_threshold:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "                \n",
    "        if best_idx is not None:\n",
    "            match_result = {\n",
    "                'index': best_idx, \n",
    "                'score': best_score,\n",
    "                'feature_sim': sim_features,\n",
    "                'spatial_sim': sim_spatial\n",
    "            }\n",
    "            self.tracking_history.append(match_result)\n",
    "            return match_result\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def update_target(self, new_features, new_bbox, confidence_score):\n",
    "        \"\"\"Update target with new features (moving target).\"\"\"\n",
    "        if not self.config.TRACKING_CONFIG['moving_target']:\n",
    "            return  # Moving target disabled\n",
    "            \n",
    "        confidence_threshold = self.config.TRACKING_CONFIG['confidence_threshold']\n",
    "        if confidence_score < confidence_threshold:\n",
    "            return  # Not confident enough to update\n",
    "            \n",
    "        momentum = self.config.TRACKING_CONFIG['update_momentum']\n",
    "        \n",
    "        # Weighted update - keeps some \"memory\" of original target\n",
    "        self.target_features = (1 - momentum) * self.target_features + momentum * new_features\n",
    "        self.target_bbox = (1 - momentum) * self.target_bbox + momentum * new_bbox\n",
    "        \n",
    "\n",
    "    \n",
    "    def _bbox3d_iou(self, b1, b2):\n",
    "        \"\"\"Calculate 3D IoU between two bounding boxes.\"\"\"\n",
    "        b1_min, b1_max = b1[0], b1[1]\n",
    "        b2_min, b2_max = b2[0], b2[1]\n",
    "\n",
    "        x_min = max(b1_min[0], b2_min[0])\n",
    "        y_min = max(b1_min[1], b2_min[1])\n",
    "        z_min = max(b1_min[2], b2_min[2])\n",
    "        x_max = min(b1_max[0], b2_max[0])\n",
    "        y_max = min(b1_max[1], b2_max[1])\n",
    "        z_max = min(b1_max[2], b2_max[2])\n",
    "        \n",
    "        if x_min >= x_max or y_min >= y_max or z_min >= z_max:\n",
    "            return 0.0\n",
    "\n",
    "        inter_vol = (x_max - x_min) * (y_max - y_min) * (z_max - z_min)\n",
    "        vol1 = (b1_max - b1_min).prod()\n",
    "        vol2 = (b2_max - b2_min).prod()\n",
    "        \n",
    "        return inter_vol / (vol1 + vol2 - inter_vol + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35970090",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9fe462f",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6da01",
   "metadata": {},
   "source": [
    "### SAM Proposals\n",
    "\n",
    "We filter proposals by confidence threshold and remove duplicates to get clean object candidates.\n",
    "\n",
    "Here we use SAM (Segment Anything Model) with a grid of point prompts, same approach as in Lab2 to generate object proposals.\n",
    "\n",
    "Each proposal contains:\n",
    "- **point**: Grid point that generated this proposal\n",
    "- **confidence**: SAM's confidence score for this segmentation\n",
    "- **mask**: Binary segmentation mask for the object\n",
    "- **area**: Number of pixels in the mask (for filtering small objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66af372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sam_proposals_project(image: Image.Image,\n",
    "                          sam_model,\n",
    "                          sam_processor,\n",
    "                          device: str,\n",
    "                          grid_size: int = 6,\n",
    "                          confidence_threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Generate object proposals using SAM with a grid of point prompts.\"\"\"\n",
    "    width, height = image.size\n",
    "    # Generate grid of point prompts\n",
    "    x_points = np.linspace(width * 0.1, width * 0.9, grid_size)\n",
    "    y_points = np.linspace(height * 0.1, height * 0.9, grid_size)\n",
    "    \n",
    "    proposals = []\n",
    "    processed_masks = []\n",
    "    \n",
    "    tqdm.write(f\"Generating SAM proposals with {grid_size}x{grid_size} grid...\")\n",
    "    \n",
    "    for i, x in enumerate(x_points):\n",
    "        for j, y in enumerate(y_points):\n",
    "            input_points = [[[x, y]]]\n",
    "            \n",
    "            try:\n",
    "                inputs = sam_processor(\n",
    "                    images=image,\n",
    "                    input_points=input_points,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                         for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = sam_model(**inputs)\n",
    "                \n",
    "                masks = sam_processor.image_processor.post_process_masks(\n",
    "                    outputs.pred_masks.cpu(),\n",
    "                    inputs[\"original_sizes\"].cpu(),\n",
    "                    inputs[\"reshaped_input_sizes\"].cpu()\n",
    "                )\n",
    "                \n",
    "                batch_masks = masks[0]\n",
    "                if len(batch_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                point_masks = batch_masks[0]\n",
    "                if len(point_masks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                best_mask_idx = 0  # Default to first mask\n",
    "                best_score = 0.5   # Default confidence\n",
    "                \n",
    "                if hasattr(outputs, 'iou_scores') and outputs.iou_scores is not None:\n",
    "                    try:\n",
    "                        iou_scores = outputs.iou_scores[0,0,:].cpu().numpy()\n",
    "                        if len(iou_scores) > 0:\n",
    "                            best_mask_idx = int(np.argmax(iou_scores))\n",
    "                            best_score = float(iou_scores[best_mask_idx])\n",
    "                             \n",
    "                            if best_score < confidence_threshold:\n",
    "                                continue\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                mask = point_masks[best_mask_idx]\n",
    "                if isinstance(mask, torch.Tensor):\n",
    "                    mask_np = mask.cpu().numpy().astype(bool)\n",
    "                else:\n",
    "                    mask_np = np.array(mask).astype(bool)\n",
    "                \n",
    "                # Check for duplicates\n",
    "                is_duplicate = False\n",
    "                for existing_mask in processed_masks:\n",
    "                    overlap = np.sum(mask_np & existing_mask)\n",
    "                    union = np.sum(mask_np | existing_mask)\n",
    "                    if union > 0 and overlap / union > 0.8:\n",
    "                        is_duplicate = True\n",
    "                        break\n",
    "                \n",
    "                if not is_duplicate and np.sum(mask_np) > 100:\n",
    "                    proposals.append({\n",
    "                        'mask': mask_np,\n",
    "                        'area': np.sum(mask_np),\n",
    "                        'point': [x, y],\n",
    "                        'confidence': best_score\n",
    "                    })\n",
    "                    processed_masks.append(mask_np)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    tqdm.write(f\"Generated {len(proposals)} unique segment proposals\")\n",
    "    return proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303a1f74",
   "metadata": {},
   "source": [
    "### CLIP Features\n",
    "\n",
    "These features are used for initial target selection when a text query is provided.\n",
    "\n",
    "We extract CLIP features from segmented regions to enable text-based object queries (like \"ball\" or \"sofa\").\n",
    "\n",
    "The process:\n",
    "1. **Make bounding box** of the mask\n",
    "2. **Crop the image** to the bounding box\n",
    "3. **Expand with padding** to include context around the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clip_features_from_segment(image: Image.Image,\n",
    "                                  mask: np.ndarray,\n",
    "                                  clip_model,\n",
    "                                  clip_processor,\n",
    "                                  device: str,\n",
    "                                  padding_ratio: float = 0.1) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract CLIP features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((int(x_min_exp), int(y_min_exp), int(x_max_exp), int(y_max_exp)))\n",
    "                \n",
    "        # Process with CLIP\n",
    "        inputs = clip_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "            \n",
    "            # Normalize image features\n",
    "            image_features = image_features/ image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract CLIP features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b27db39",
   "metadata": {},
   "source": [
    "### DINOv2 Features\n",
    "\n",
    "\n",
    "DINOv2 provides superior visual features for object tracking compared to CLIP, especially for appearance-based matching across frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dino_features_from_segment(\n",
    "    image: Image.Image,\n",
    "    mask: np.ndarray,\n",
    "    dino_model,\n",
    "    dino_processor,\n",
    "    device: str,\n",
    "    padding_ratio: float = 0.1\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract DINOv2 features from a segmented region.\"\"\"\n",
    "    try:\n",
    "        # Find bounding box of mask\n",
    "        coords = np.where(mask)\n",
    "        if len(coords[0]) == 0:\n",
    "            return None\n",
    "        \n",
    "        y_min, y_max = coords[0].min(), coords[0].max()\n",
    "        x_min, x_max = coords[1].min(), coords[1].max()\n",
    "        \n",
    "        # Expand bounds with padding\n",
    "        pad_y = int((y_max - y_min) * padding_ratio)\n",
    "        pad_x = int((x_max - x_min) * padding_ratio)\n",
    "\n",
    "        y_min_exp = max(int(y_min - pad_y), 0)\n",
    "        y_max_exp = min(int(y_max + pad_y), mask.shape[0])\n",
    "        x_min_exp = max(int(x_min - pad_x), 0)\n",
    "        x_max_exp = min(int(x_max + pad_x), mask.shape[1])\n",
    "\n",
    "        image_crop = image.crop((x_min_exp, y_min_exp, x_max_exp, y_max_exp))\n",
    "        \n",
    "        # Process with DINOv2\n",
    "        inputs = dino_processor(images=image_crop, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = dino_model(**inputs)\n",
    "            # Use the [CLS] token embedding or pooled output as image feature\n",
    "            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                image_features = outputs.pooler_output\n",
    "            else:\n",
    "                image_features = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "            \n",
    "            # Normalize features\n",
    "            image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features.cpu().numpy().squeeze()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract DINO features: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f814f5",
   "metadata": {},
   "source": [
    "## 3D Position Extraction\n",
    "\n",
    "To enable spatial tracking in world coordinates, we convert 2D segmentation masks into 3D object representations using camera geometry and depth data.\n",
    "\n",
    "#### **What We Extract:**\n",
    "- **Object position**: 3D centroid in world coordinates  \n",
    "- **Object extent**: Axis-aligned bounding box for size/shape information\n",
    "- **Spatial context**: Used for 3D IoU calculations in tracking similarity\n",
    "\n",
    "#### **The Pipeline:**\n",
    "\n",
    "**1. Pixel Sampling**\n",
    "- **Sample up to 500 pixels** from the mask for computational efficiency\n",
    "- **Filter minimum size**: Skip masks with fewer than 20 pixels (noise rejection)\n",
    "\n",
    "**2. Depth Processing** \n",
    "- **Extract depth values** at sampled pixel locations\n",
    "- **Filter valid depths**: Remove pixels with depth ≤ 0 (invalid/missing data)\n",
    "\n",
    "**3. 3D Reconstruction**\n",
    "- **Back-project to camera space**: Use intrinsics (fx, fy, cx, cy) to convert pixels → 3D points\n",
    "- **Transform to world space**: Apply camera pose matrix for global coordinates\n",
    "\n",
    "**4. Compact Representation**\n",
    "- **Centroid**: Mean position of all valid 3D points\n",
    "- **Bounding box**: [min_xyz, max_xyz] extent for spatial similarity\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Efficient**: Sampling limits computation while preserving shape information\n",
    "- **Robust**: Filters invalid depth data that could corrupt 3D positions  \n",
    "- **Tracking-ready**: Compact format perfect for 3D IoU calculations between frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb948f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_3d_position_from_mask(\n",
    "    mask: np.ndarray,\n",
    "    depth_image: np.ndarray,\n",
    "    camera_intrinsics: np.ndarray,\n",
    "    camera_pose: np.ndarray,\n",
    "    depth_scale: float,\n",
    "    max_points: int = 500\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Convert a binary mask into compact 3D object representation:\n",
    "    centroid + bounding box in world coordinates.\n",
    "\n",
    "    Args:\n",
    "        mask: Binary segmentation mask (H,W)\n",
    "        depth_image: Depth map aligned with RGB (H,W), uint16 or float\n",
    "        camera_intrinsics\n",
    "        camera_pose\n",
    "        depth_scale: scale factor for depth values\n",
    "        max_points: number of pixels to sample from mask\n",
    "\n",
    "    Returns:\n",
    "        dict with 'centroid' and 'bbox_3d', or None if invalid\n",
    "    \"\"\"\n",
    "    coords = np.column_stack(np.where(mask))  # (y, x) pixel indices\n",
    "    if coords.shape[0] < 20:\n",
    "        return None\n",
    "\n",
    "    # Sample pixels if mask too large\n",
    "    sample_idx = np.random.choice(len(coords), min(max_points, len(coords)), replace=False)\n",
    "    coords = coords[sample_idx]\n",
    "\n",
    "    # Depth values\n",
    "    z = depth_image[coords[:, 0], coords[:, 1]].astype(np.float32) / depth_scale\n",
    "    valid = z > 0\n",
    "    if not np.any(valid):\n",
    "        return None\n",
    "\n",
    "    coords = coords[valid]\n",
    "    z = z[valid]\n",
    "\n",
    "    # Get camera parameters\n",
    "    fx, fy = camera_intrinsics[0, 0], camera_intrinsics[1, 1]\n",
    "    cx, cy = camera_intrinsics[0, 2], camera_intrinsics[1, 2]\n",
    "    \n",
    "    x = (coords[:, 1] - cx) * z / fx\n",
    "    y = (coords[:, 0] - cy) * z / fy\n",
    "    pts_cam = np.stack([x, y, z], axis=-1)\n",
    "\n",
    "    # Camera → World\n",
    "    pts_h = np.concatenate([pts_cam, np.ones((pts_cam.shape[0], 1))], axis=-1).T\n",
    "    pts_world = (camera_pose @ pts_h)[:3].T\n",
    "\n",
    "    # Compact representation\n",
    "    centroid = pts_world.mean(axis=0)\n",
    "    bbox_min = pts_world.min(axis=0)\n",
    "    bbox_max = pts_world.max(axis=0)\n",
    "\n",
    "    return {\n",
    "        'centroid': centroid,\n",
    "        'bbox_3d': np.stack([bbox_min, bbox_max])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c944aa7",
   "metadata": {},
   "source": [
    "## Frame Processing\n",
    "\n",
    "Each processed frame contains a list of detected objects with their visual features, spatial location, and metadata.\n",
    "\n",
    "This is where we put it all together for each frame:\n",
    "\n",
    "1. **Load RGB and depth images** for the frame\n",
    "\n",
    "2. **Generate SAM proposals** using the grid-based approach\n",
    "3. **Extract features** for each proposal (both DINOv2 and CLIP)\n",
    "4. **Calculate 3D positions** from masks and depth data\n",
    "5. **Package results** into a structured format for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_project(frame_data: Dict,\n",
    "        sam_model, sam_processor,\n",
    "        dino_model, dino_processor,\n",
    "        clip_model, clip_processor,\n",
    "        device: str,\n",
    "        config: Config,\n",
    "        grid_size: int = 6) -> Dict:\n",
    "    \"\"\"Process a single frame for Project: SAM proposals + CLIP embeddings + DINOv2 features.\"\"\"\n",
    "    results = {\n",
    "        'frame_name': frame_data['frame_name'],\n",
    "        'objects': []\n",
    "    }\n",
    "    try:\n",
    "        # Load images\n",
    "        image = Image.open(frame_data['rgb_path']).convert(\"RGB\")\n",
    "        depth_image = cv2.imread(frame_data['depth_path'], cv2.IMREAD_UNCHANGED)\n",
    "        rgb_image = np.array(image)\n",
    "        \n",
    "        # Generate SAM proposals\n",
    "        proposals = generate_sam_proposals_project(\n",
    "            image,\n",
    "            sam_model,\n",
    "            sam_processor,\n",
    "            device,\n",
    "            grid_size=grid_size,\n",
    "            confidence_threshold=config.PROJECT_CONFIG['sam_confidence_threshold']\n",
    "        )\n",
    "        \n",
    "        # Process each proposal\n",
    "        for proposal in proposals:\n",
    "            mask = proposal['mask']\n",
    "            \n",
    "            # Extract DINOv2 features\n",
    "            dino_features = extract_dino_features_from_segment(\n",
    "                image,\n",
    "                mask,\n",
    "                dino_model,\n",
    "                dino_processor,\n",
    "                device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops'] \n",
    "            )\n",
    "            \n",
    "            if dino_features is None:\n",
    "                continue\n",
    "\n",
    "            clip_features = extract_clip_features_from_segment(\n",
    "                image, mask, clip_model, clip_processor, device,\n",
    "                padding_ratio=config.PROJECT_CONFIG['padding_ratio_image_crops']\n",
    "            )\n",
    "            \n",
    "            # Get compact 3D position (Bounding 3D box + Centroid)\n",
    "            pos_data = extract_3d_position_from_mask(\n",
    "                mask,\n",
    "                depth_image,\n",
    "                frame_data['camera_intrinsics'],\n",
    "                frame_data['camera_pose'],\n",
    "                config.TSDF_CONFIG['depth_scale']\n",
    "            )\n",
    "            if pos_data is None:\n",
    "                continue\n",
    "            \n",
    "            results['objects'].append({\n",
    "                'features': dino_features,\n",
    "                'clip_features': clip_features,\n",
    "                'centroid': pos_data['centroid'],\n",
    "                'bbox_3d': pos_data['bbox_3d'],\n",
    "                'area_px': proposal['area'],\n",
    "                'confidence': proposal['confidence']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_data['frame_name']}: {e}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a826da",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera poses\n",
    "camera_poses = load_camera_poses(config.TRAJ_FILE_PATH)\n",
    "\n",
    "def get_frame_list(config, frame_skip=1, max_frames=25, start_frame_name=None):\n",
    "    files = sorted(os.listdir(config.RGB_PATH))\n",
    "    frames = []\n",
    "\n",
    "    # If start_frame_name is provided, find its index\n",
    "    if start_frame_name is not None:\n",
    "        try:\n",
    "            start_idx = next(i for i, f in enumerate(files) if f == start_frame_name)\n",
    "            files = files[start_idx:]  # start from that frame\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"Start frame {start_frame_name} not found in {config.RGB_PATH}\")\n",
    "\n",
    "    for i, f in enumerate(files[::frame_skip]):\n",
    "        if i >= max_frames:\n",
    "            break\n",
    "\n",
    "        timestamp = os.path.splitext(f)[0]\n",
    "\n",
    "        # Load intrinsics for this frame\n",
    "        try:\n",
    "            K, image_size = load_camera_intrinsics(config.INTRINSICS_PATH, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}, failed to load intrinsics: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Load camera pose\n",
    "        pose = camera_poses.get(timestamp, np.eye(4))\n",
    "\n",
    "        frames.append({\n",
    "            \"frame_name\": f,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"rgb_path\": os.path.join(config.RGB_PATH, f),\n",
    "            \"depth_path\": os.path.join(config.DEPTH_PATH, f.replace(\".jpg\", \".png\")),\n",
    "            \"camera_intrinsics\": K,\n",
    "            \"intrinsics_size\": image_size,\n",
    "            \"camera_pose\": pose\n",
    "        })\n",
    "\n",
    "    return frames\n",
    "\n",
    "def project_points(K, points_3d):\n",
    "    \"\"\"Project 3D points to 2D for visualization.\"\"\"\n",
    "    uv_coords = []\n",
    "    for X, Y, Z in points_3d:\n",
    "        if Z > 0:\n",
    "            uv = K @ np.array([X, Y, Z])\n",
    "            u, v = uv[0] / uv[2], uv[1] / uv[2]\n",
    "            uv_coords.append((u, v))\n",
    "    return uv_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8297e2",
   "metadata": {},
   "source": [
    "## High-Level Interface Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa443b0",
   "metadata": {},
   "source": [
    "### Frame Analysis\n",
    "\n",
    "The `analyse_frames()` function processes a sequence of frames to extract objects and their features for tracking.\n",
    "\n",
    "#### **Configuration:**\n",
    "- **n_frames**: Number of frames to process (default: 30)\n",
    "- **start_frame**: Starting frame filename (e.g., \"47333473_58534.757.png\")\n",
    "- **frame_skip**: Uses config setting to skip frames for efficiency\n",
    "\n",
    "#### **Processing Pipeline:**\n",
    "1. **Load models once** - SAM, DINOv2, and CLIP models loaded efficiently at startup\n",
    "2. **Get frame metadata** - Camera poses, intrinsics, and file paths for each frame\n",
    "3. **Process each frame** - Run the complete pipeline:\n",
    "   - Generate SAM object proposals\n",
    "   - Extract DINOv2 features for tracking\n",
    "   - Extract CLIP features for text queries\n",
    "   - Calculate 3D positions from depth data\n",
    "4. **Package results** - Structured data ready for the ObjectTracker\n",
    "\n",
    "#### **Output:**\n",
    "- **frame_results**: List of frames, each containing detected objects with features and 3D positions\n",
    "- **device**: GPU device handle for subsequent tracking operations\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Model reuse**: Load expensive models once, not per frame\n",
    "- **Structured output**: Consistent format for all tracking functions\n",
    "- **Flexible sequences**: Start from any frame, process any number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_frames(config, n_frames=30, start_frame=\"47333473_58547.751.png\"):\n",
    "    \"\"\"\n",
    "    Extract objects from multiple frames using SAM + DINO + CLIP.\n",
    "    \"\"\"\n",
    "    # Load models\n",
    "    sam_model, sam_processor, device = load_sam_model(model_size='base')\n",
    "    dino_model, dino_processor, _ = load_dino_model(device=device)\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "\n",
    "    # Get frame metadata\n",
    "    frames_metadata = get_frame_list(config, frame_skip=1, max_frames=n_frames, start_frame_name=start_frame)\n",
    "\n",
    "    all_results = []\n",
    "    for frame_data in tqdm(frames_metadata, desc=\"Processing frames\"):\n",
    "        results = process_frame_project(\n",
    "            frame_data,\n",
    "            sam_model, sam_processor,\n",
    "            dino_model, dino_processor,\n",
    "            clip_model, clip_processor,\n",
    "            device,\n",
    "            config\n",
    "        )\n",
    "        results[\"frame_data\"] = frame_data\n",
    "        all_results.append(results)\n",
    "\n",
    "    return all_results, device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17b641",
   "metadata": {},
   "source": [
    "### Tracking with Slider (Refactored)\n",
    "\n",
    "**Interactive Visualization**: The `query_with_slider()` function implements the complete tracking pipeline with an interactive slider for visual assessment. Your eyes are the ground truth here!\n",
    "\n",
    "#### **How It Works:**\n",
    "\n",
    "**1. Target Selection**\n",
    "- **With text query**: Uses CLIP similarity to find best matching object (e.g., \"ball\", \"sofa\")\n",
    "- **Without query**: Selects largest object (most pixels) for better tracking stability\n",
    "\n",
    "**2. Tracking Process**\n",
    "- **Initialize tracker** with target features and 3D bounding box from selected object\n",
    "- **For each frame**: Find best match using combined DINOv2 feature + 3D spatial similarity\n",
    "- **Update target** (if moving_target enabled) using exponential moving average for adaptation\n",
    "\n",
    "**3. Visualization**\n",
    "- **Bounding boxes** are projected from 3D world coordinates to 2D image space\n",
    "- **Interactive slider** lets you browse through frames to visually assess tracking quality\n",
    "- **Red rectangles** show the tracked object's projected 3D bounding box\n",
    "\n",
    "#### **Why This Approach:**\n",
    "- **Visual assessment** is more reliable than automated metrics without ground truth\n",
    "- **Interactive exploration** lets you see exactly when tracking succeeds or fails\n",
    "- **3D-to-2D projection** provides spatial context from the world coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_slider(frame_results, device=None, query=None):\n",
    "    \"\"\"Track objects through frames with interactive slider - now using ObjectTracker class.\"\"\"\n",
    "    \n",
    "    if not frame_results:\n",
    "        print(\"Error: No frame results provided\")\n",
    "        return\n",
    "    \n",
    "    if not any(len(r['objects']) > 0 for r in frame_results):\n",
    "        print(\"Error: No objects found in any frame\")\n",
    "        return\n",
    "\n",
    "    if device is None:\n",
    "        _, _, device = load_sam_model(model_size='base')\n",
    "\n",
    "    clip_model, clip_processor, _ = load_clip_model(device=device)\n",
    "    \n",
    "    # Initialize tracker\n",
    "    tracker = ObjectTracker(config)\n",
    "\n",
    "    # Target selection logic\n",
    "    text_features_np = None\n",
    "    if query is not None:\n",
    "        text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = clip_model.get_text_features(**text_inputs)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features_np = text_features.cpu().numpy().squeeze()\n",
    "\n",
    "    # Pick object to track\n",
    "    if query is None or text_features_np is None:            \n",
    "        # Pick an object (first object with largest area for better tracking)\n",
    "        best_object_id, best_frame_id = 0, 0\n",
    "        best_area = 0\n",
    "        \n",
    "        # Look for substantial object in first few frames\n",
    "        for f_idx in range(min(3, len(frame_results))):\n",
    "            for o_idx, obj in enumerate(frame_results[f_idx]['objects']):\n",
    "                if obj['area_px'] > best_area:\n",
    "                    best_area = obj['area_px']\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "        \n",
    "    else:\n",
    "        # Pick object by CLIP similarity with query\n",
    "        best_score_overall = -1\n",
    "        best_frame_id, best_object_id = None, None\n",
    "        for f_idx, results in enumerate(frame_results):\n",
    "            for o_idx, obj in enumerate(results['objects']):\n",
    "                if obj['clip_features'] is None:\n",
    "                    continue\n",
    "                sim = float(np.dot(obj['clip_features'], text_features_np))\n",
    "                if sim > best_score_overall:\n",
    "                    best_score_overall = sim\n",
    "                    best_frame_id, best_object_id = f_idx, o_idx\n",
    "\n",
    "        if best_score_overall < 0:\n",
    "            print(\"No suitable object found for query.\")\n",
    "            return\n",
    "\n",
    "    # Set initial target\n",
    "    target_features = frame_results[best_frame_id]['objects'][best_object_id]['features']\n",
    "    target_bbox = frame_results[best_frame_id]['objects'][best_object_id]['bbox_3d']\n",
    "    tracker.set_target(target_features, target_bbox)\n",
    "    \n",
    "    print(f\"Starting tracking from frame {best_frame_id}, object {best_object_id}\")\n",
    "    print(f\"Tracking config: moving_target={config.TRACKING_CONFIG['moving_target']}, threshold={config.TRACKING_CONFIG['similarity_threshold']}\")\n",
    "\n",
    "    # Process frames with tracker\n",
    "    processed_frames = []\n",
    "    print(f\"Processing frames ...\")\n",
    "    \n",
    "    for idx, results in enumerate(frame_results):\n",
    "        frame = Image.open(results['frame_data']['rgb_path']).convert(\"RGB\")\n",
    "        overlay = frame.convert('RGBA')\n",
    "        draw = ImageDraw.Draw(overlay, 'RGBA')\n",
    "\n",
    "        if results['objects']:\n",
    "            # Use tracker to find best match\n",
    "            match = tracker.find_best_match(results['objects'])\n",
    "            if match:\n",
    "                best_idx = match['index']\n",
    "                obj = results['objects'][best_idx]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                points_2d = project_points(results['frame_data']['camera_intrinsics'], obj['bbox_3d'])\n",
    "                if points_2d:\n",
    "                    xs, ys = zip(*points_2d)\n",
    "                    x0, x1 = int(min(xs)), int(max(xs))\n",
    "                    y0, y1 = int(min(ys)), int(max(ys))\n",
    "                    draw.rectangle([x0, y0, x1, y1], outline=(255,0,0,180), width=3)\n",
    "                \n",
    "                # Update target if moving target is enabled\n",
    "                tracker.update_target(obj['features'], obj['bbox_3d'], match['score'])\n",
    "\n",
    "        processed_frames.append(overlay)\n",
    "\n",
    "\n",
    "\n",
    "    # Create slider widget\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(processed_frames)-1,\n",
    "        step=1,\n",
    "        description='Frame'\n",
    "    )\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def show_frame(change):\n",
    "        idx = change['new']\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            display(processed_frames[idx])\n",
    "\n",
    "    slider.observe(show_frame, names='value')\n",
    "    display(widgets.VBox([slider, out]))\n",
    "    \n",
    "    # Show first frame by default\n",
    "    with out:\n",
    "        display(processed_frames[0])\n",
    "    \n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4e5fd",
   "metadata": {},
   "source": [
    "## Examples and Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0019f",
   "metadata": {},
   "source": [
    "### Example 1: Track with Fixed Target (Default)\n",
    "\n",
    "**Trade-off**: May lose track if the object's appearance changes significantly, but won't accidentally switch to tracking a different object.\n",
    "\n",
    "This demonstrates the basic tracking approach where the target features **never change** after initialization.\n",
    "\n",
    "**Use case**: When you want the most stable tracking that won't drift to other objects, even if the appearance changes due to lighting or viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract objects from frames\n",
    "frame_results, device_id = analyse_frames(config, n_frames=20, start_frame=\"47333473_58534.757.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aab30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track with query (fixed target)\n",
    "print(\"=== Tracking with Fixed Target ===\")\n",
    "tracker_fixed = query_with_slider(frame_results, device=None, query=\"ball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22c7cf",
   "metadata": {},
   "source": [
    "### Example 2: Track with Moving Average Target\n",
    "\n",
    "**Trade-off**: Better handles appearance changes, but small risk of gradually drifting to a different object.\n",
    "\n",
    "Here we enable adaptive tracking where the target features **gradually update** based on successful matches.\n",
    "\n",
    "**Use case**: When lighting, shadows, or viewing angles change significantly across frames. The tracker adapts while maintaining object identity.\n",
    "\n",
    "**Key parameters:**\n",
    "\n",
    "- `moving_target = True`: Enable feature updates- `confidence_threshold = 0.8`: Only update when very confident\n",
    "- `update_momentum = 0.1`: Conservative update (10% new, 90% existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57133979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable moving target\n",
    "config.TRACKING_CONFIG['moving_target'] = True\n",
    "config.TRACKING_CONFIG['update_momentum'] = 0.1  # Conservative update\n",
    "config.TRACKING_CONFIG['confidence_threshold'] = 0.8\n",
    "\n",
    "print(\"=== Tracking with Moving Target ===\")\n",
    "tracker_moving = query_with_slider(frame_results, device=None, query=\"ball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba6d86",
   "metadata": {},
   "source": [
    "### Example 3: Compare Tracking Settings\n",
    "\n",
    "**Compare by**: Using the sliders to see when each approach loses/maintains tracking. Look for tracking gaps, bounding box jumps, or switches to wrong objects.\n",
    "\n",
    "This example shows how different similarity thresholds affect tracking behavior. Each threshold gets its own interactive slider for direct comparison.\n",
    "\n",
    "- **0.8 (Strict)**: Only high-confidence matches → fewer false positives but may lose track more easily\n",
    "\n",
    "**Similarity Thresholds:**- **0.6 (Balanced)**: Moderate threshold → good balance of continuity and accuracy  \n",
    "- **0.4 (Permissive)**: Accepts lower-confidence matches → more continuous tracking but higher false positive risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8535615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to fixed target for comparison\n",
    "config.TRACKING_CONFIG['moving_target'] = False\n",
    "\n",
    "# Try different similarity thresholds with separate sliders\n",
    "print(\"=== Comparing Different Similarity Thresholds ===\")\n",
    "\n",
    "original_threshold = config.TRACKING_CONFIG['similarity_threshold']\n",
    "\n",
    "# Threshold 0.4 - Permissive\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.4\n",
    "print(\"\\n--- Threshold 0.4 (Permissive) ---\")\n",
    "tracker_04 = query_with_slider(frame_results, device=None, query=\"ball\")\n",
    "\n",
    "# Threshold 0.6 - Balanced  \n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.6\n",
    "print(\"\\n--- Threshold 0.6 (Balanced) ---\")\n",
    "tracker_06 = query_with_slider(frame_results, device=None, query=\"ball\")\n",
    "\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = original_threshold\n",
    "\n",
    "# Threshold 0.8 - Strict# Reset to original\n",
    "\n",
    "config.TRACKING_CONFIG['similarity_threshold'] = 0.8\n",
    "\n",
    "print(\"\\n--- Threshold 0.8 (Strict) ---\")\n",
    "tracker_08 = query_with_slider(frame_results, device=None, query=\"ball\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74ad2",
   "metadata": {},
   "source": [
    "### Example 4: Track without Query (Largest Object)\n",
    "\n",
    "**Use case**: Exploratory analysis when you don't know what objects are present or want to track the most prominent object in the scene.\n",
    "\n",
    "When no text query is provided, the tracker automatically selects the largest detected object (by pixel area) as the target.\n",
    "\n",
    "- **Better for furniture**: In indoor scenes, large objects like sofas/tables are good tracking targets\n",
    "\n",
    "**Why largest object?**- **Less noise**: Larger masks are less likely to be false detections\n",
    "- **More stable tracking**: Larger objects have more feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track without specific query - will pick largest object\n",
    "print(\"=== Tracking Largest Object (No Query) ===\")\n",
    "tracker_no_query = query_with_slider(frame_results, device=None, query=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a873f",
   "metadata": {},
   "source": [
    "## Configuration Toggle Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0913811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current configuration\n",
    "print(\"Current Tracking Configuration:\")\n",
    "for key, value in config.TRACKING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nTo modify tracking behavior:\")\n",
    "print(\"  config.TRACKING_CONFIG['moving_target'] = True/False\")\n",
    "print(\"  config.TRACKING_CONFIG['similarity_threshold'] = 0.4-0.9\")\n",
    "print(\"  config.TRACKING_CONFIG['feature_weight'] = 0.5-0.9 (vs spatial weight)\")\n",
    "print(\"  config.TRACKING_CONFIG['update_momentum'] = 0.05-0.3 (if moving_target=True)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
